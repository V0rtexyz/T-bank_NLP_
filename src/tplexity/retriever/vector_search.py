import asyncio
import logging
import traceback
from typing import Literal
from uuid import uuid4

import httpx
from qdrant_client import AsyncQdrantClient
from qdrant_client.models import (
    Distance,
    Fusion,
    FusionQuery,
    Modifier,
    PointIdsList,
    PointStruct,
    Prefetch,
    SparseVectorParams,
    VectorParams,
)

from tplexity.retriever.config import settings
from tplexity.retriever.dense_embedding import get_embedding_model
from tplexity.retriever.retry_utils import retry_with_backoff
from tplexity.retriever.sparse_embedding import get_bm25_model

logger = logging.getLogger(__name__)


class VectorSearch:
    """–ö–ª–∞—Å—Å –¥–ª—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ —á–µ—Ä–µ–∑ Qdrant —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π dense –∏ sparse –≤–µ–∫—Ç–æ—Ä–æ–≤"""

    def __init__(
        self,
        collection_name: str,
        host: str,
        port: int,
        api_key: str | None,
        timeout: int,
        prefetch_ratio: float,
        connect_timeout: int | None = None,
        read_timeout: int | None = None,
        write_timeout: int | None = None,
        pool_connections: int | None = None,
        pool_maxsize: int | None = None,
        max_keepalive_connections: int | None = None,
        keepalive_expiry: float | None = None,
        max_retries: int | None = None,
        retry_initial_delay: float | None = None,
        retry_max_delay: float | None = None,
        retry_exponential_base: float | None = None,
        retry_jitter: bool | None = None,
    ):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–æ–≤–∏–∫–∞

        Args:
            collection_name (str): –ò–º—è –∫–æ–ª–ª–µ–∫—Ü–∏–∏ –≤ Qdrant
            host (str): –•–æ—Å—Ç Qdrant
            port (int): –ü–æ—Ä—Ç Qdrant
            api_key (str | None): API –∫–ª—é—á –¥–ª—è Qdrant
            timeout (int): –û–±—â–∏–π —Ç–∞–π–º–∞—É—Ç –¥–ª—è –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è
            prefetch_ratio (float): –í–æ —Å–∫–æ–ª—å–∫–æ —Ä–∞–∑ –±–æ–ª—å—à–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è prefetch
            connect_timeout (int | None): –¢–∞–π–º–∞—É—Ç –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
            read_timeout (int | None): –¢–∞–π–º–∞—É—Ç —á—Ç–µ–Ω–∏—è –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
            write_timeout (int | None): –¢–∞–π–º–∞—É—Ç –∑–∞–ø–∏—Å–∏ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
            pool_connections (int | None): –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π –≤ –ø—É–ª–µ
            pool_maxsize (int | None): –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –ø—É–ª–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π
            max_keepalive_connections (int | None): –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ keepalive —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π
            keepalive_expiry (float | None): –í—Ä–µ–º—è –∂–∏–∑–Ω–∏ keepalive —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
            max_retries (int | None): –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫ retry
            retry_initial_delay (float | None): –ù–∞—á–∞–ª—å–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –¥–ª—è retry –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
            retry_max_delay (float | None): –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –¥–ª—è retry –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
            retry_exponential_base (float | None): –ë–∞–∑–∞ –¥–ª—è exponential backoff
            retry_jitter (bool | None): –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–∏ jitter –¥–ª—è retry
        """
        self.collection_name = collection_name
        self.host = host
        self.port = port
        self.api_key = api_key
        self.timeout = timeout
        self.prefetch_ratio = prefetch_ratio

        # Retry –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        self.max_retries = max_retries or settings.qdrant_max_retries
        self.retry_initial_delay = retry_initial_delay or settings.qdrant_retry_initial_delay
        self.retry_max_delay = retry_max_delay or settings.qdrant_retry_max_delay
        self.retry_exponential_base = retry_exponential_base or settings.qdrant_retry_exponential_base
        self.retry_jitter = retry_jitter if retry_jitter is not None else settings.qdrant_retry_jitter

        logger.info("üîÑ [retriever][vector_search] –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∏–µ–Ω—Ç–∞ Qdrant —Å connection pooling")
        try:
            # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Ç–∞–π–º–∞—É—Ç–æ–≤
            connect_timeout_val = connect_timeout or settings.qdrant_connect_timeout
            read_timeout_val = read_timeout or settings.qdrant_read_timeout
            write_timeout_val = write_timeout or settings.qdrant_write_timeout

            timeout_config = httpx.Timeout(
                connect=connect_timeout_val,
                read=read_timeout_val,
                write=write_timeout_val,
                pool=timeout,
            )

            # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ connection pooling
            pool_connections_val = pool_connections or settings.qdrant_pool_connections
            pool_maxsize_val = pool_maxsize or settings.qdrant_pool_maxsize
            max_keepalive_val = max_keepalive_connections or settings.qdrant_max_keepalive_connections
            keepalive_expiry_val = keepalive_expiry or settings.qdrant_keepalive_expiry

            limits = httpx.Limits(
                max_connections=pool_maxsize_val,
                max_keepalive_connections=max_keepalive_val,
                keepalive_expiry=keepalive_expiry_val,
            )

            # –°–æ–∑–¥–∞–µ–º –∫–∞—Å—Ç–æ–º–Ω—ã–π httpx –∫–ª–∏–µ–Ω—Ç —Å connection pooling
            httpx_client = httpx.AsyncClient(
                timeout=timeout_config,
                limits=limits,
                http2=True,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º HTTP/2 –¥–ª—è –ª—É—á—à–µ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
            )

            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º Qdrant –∫–ª–∏–µ–Ω—Ç —Å –∫–∞—Å—Ç–æ–º–Ω—ã–º httpx –∫–ª–∏–µ–Ω—Ç–æ–º
            # AsyncQdrantClient –∏—Å–ø–æ–ª—å–∑—É–µ—Ç httpx –≤–Ω—É—Ç—Ä–∏ –∏ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –ø–µ—Ä–µ–¥–∞—á—É –∫–∞—Å—Ç–æ–º–Ω–æ–≥–æ –∫–ª–∏–µ–Ω—Ç–∞
            # —á–µ—Ä–µ–∑ –ø–∞—Ä–∞–º–µ—Ç—Ä http_client (–≤ –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö –≤–µ—Ä—Å–∏—è—Ö –º–æ–∂–µ—Ç –±—ã—Ç—å httpx_client)
            try:
                self.client = AsyncQdrantClient(
                    url=f"https://{self.host}:{self.port}",
                    api_key=self.api_key,
                    timeout=timeout,
                    http_client=httpx_client,  # –ü—Ä–æ–±—É–µ–º http_client
                )
            except TypeError:
                # –ï—Å–ª–∏ http_client –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è, –ø—Ä–æ–±—É–µ–º httpx_client
                try:
                    self.client = AsyncQdrantClient(
                        url=f"https://{self.host}:{self.port}",
                        api_key=self.api_key,
                        timeout=timeout,
                        httpx_client=httpx_client,
                    )
                except TypeError:
                    # –ï—Å–ª–∏ –æ–±–∞ –Ω–µ —Ä–∞–±–æ—Ç–∞—é—Ç, —Å–æ–∑–¥–∞–µ–º –∫–ª–∏–µ–Ω—Ç –±–µ–∑ –∫–∞—Å—Ç–æ–º–Ω–æ–≥–æ httpx
                    # Connection pooling –≤—Å–µ —Ä–∞–≤–Ω–æ –±—É–¥–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å —á–µ—Ä–µ–∑ –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π httpx
                    logger.warning(
                        "‚ö†Ô∏è [retriever][vector_search] –ö–∞—Å—Ç–æ–º–Ω—ã–π httpx –∫–ª–∏–µ–Ω—Ç –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è, "
                        "–∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π connection pooling"
                    )
                    self.client = AsyncQdrantClient(
                        url=f"https://{self.host}:{self.port}",
                        api_key=self.api_key,
                        timeout=timeout,
                    )

            logger.info(
                f"‚úÖ [retriever][vector_search] –ö–ª–∏–µ–Ω—Ç Qdrant –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω: {self.host}:{self.port} "
                f"(pool: {pool_connections_val}/{pool_maxsize_val}, keepalive: {max_keepalive_val}, "
                f"timeouts: connect={connect_timeout_val}s, read={read_timeout_val}s, write={write_timeout_val}s)"
            )
        except Exception as e:
            error_traceback = traceback.format_exc()
            logger.error(
                f"‚ùå [retriever][vector_search] –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –∫–ª–∏–µ–Ω—Ç–∞ Qdrant: {e}\n{error_traceback}",
                exc_info=True,
            )
            raise

        self.embedding_model = get_embedding_model()
        self.embedding_dim = self.embedding_model.get_sentence_embedding_dimension()
        logger.info(f"‚úÖ [retriever][vector_search] Dense –º–æ–¥–µ–ª—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞, —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: {self.embedding_dim}")

        self.bm25 = get_bm25_model()
        logger.info("‚úÖ [retriever][vector_search] BM25 –º–æ–¥–µ–ª—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞")

    async def _ensure_collection(self) -> None:
        """–°–æ–∑–¥–∞—Ç—å –∫–æ–ª–ª–µ–∫—Ü–∏—é —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π dense –∏ sparse –≤–µ–∫—Ç–æ—Ä–æ–≤, –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç"""
        async def _get_collections_operation():
            return await self.client.get_collections()

        try:
            collections = await retry_with_backoff(
                _get_collections_operation,
                max_retries=self.max_retries,
                initial_delay=self.retry_initial_delay,
                max_delay=self.retry_max_delay,
                exponential_base=self.retry_exponential_base,
                jitter=self.retry_jitter,
            )
        except Exception as e:
            error_traceback = traceback.format_exc()
            logger.error(
                f"‚ùå [retriever][vector_search] –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Å–ø–∏—Å–∫–∞ –∫–æ–ª–ª–µ–∫—Ü–∏–π: {type(e).__name__}: {e}\n{error_traceback}",
                exc_info=True,
            )
            raise

        collection_names = [col.name for col in collections.collections]

        if self.collection_name not in collection_names:
            vectors_config = {
                "dense": VectorParams(
                    size=self.embedding_dim,
                    distance=Distance.COSINE,
                )
            }

            sparse_vectors_config = {
                "bm25": SparseVectorParams(
                    modifier=Modifier.IDF,
                ),
            }

            async def _create_collection_operation():
                return await self.client.create_collection(
                    collection_name=self.collection_name,
                    vectors_config=vectors_config,
                    sparse_vectors_config=sparse_vectors_config,
                )

            try:
                await retry_with_backoff(
                    _create_collection_operation,
                    max_retries=self.max_retries,
                    initial_delay=self.retry_initial_delay,
                    max_delay=self.retry_max_delay,
                    exponential_base=self.retry_exponential_base,
                    jitter=self.retry_jitter,
                )
                logger.info(
                    f"‚úÖ [retriever][vector_search] –ö–æ–ª–ª–µ–∫—Ü–∏—è {self.collection_name} —Å–æ–∑–¥–∞–Ω–∞ —Å dense –∏ sparse –≤–µ–∫—Ç–æ—Ä–∞–º–∏"
                )
            except Exception as e:
                error_traceback = traceback.format_exc()
                logger.error(
                    f"‚ùå [retriever][vector_search] –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –∫–æ–ª–ª–µ–∫—Ü–∏–∏: {type(e).__name__}: {e}\n{error_traceback}",
                    exc_info=True,
                )
                raise
        else:
            logger.info(f"‚úÖ [retriever][vector_search] –ö–æ–ª–ª–µ–∫—Ü–∏—è {self.collection_name} —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç")

    async def add_documents(
        self,
        documents: list[str],
        ids: list[str] | None = None,
        metadatas: list[dict] | None = None,
    ) -> None:
        """
        –î–æ–±–∞–≤–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç—ã –≤ –≤–µ–∫—Ç–æ—Ä–Ω—É—é –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö —Å dense –∏ sparse –≤–µ–∫—Ç–æ—Ä–∞–º–∏

        Args:
            documents (list[str]): –°–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è
            ids (list[str] | None): –°–ø–∏—Å–æ–∫ ID –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤. –ï—Å–ª–∏ None, –≥–µ–Ω–µ—Ä–∏—Ä—É—é—Ç—Å—è UUID
            metadatas (list[dict] | None): –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞

        Raises:
            ValueError: –ï—Å–ª–∏ –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –Ω–µ–≤–∞–ª–∏–¥–Ω—ã
        """
        if not documents:
            raise ValueError("–°–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –Ω–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –ø—É—Å—Ç—ã–º")

        if metadatas is None:
            metadatas = [{}] * len(documents)

        if len(metadatas) != len(documents):
            raise ValueError(
                f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö ({len(metadatas)}) –Ω–µ —Å–æ–≤–ø–∞–¥–∞–µ—Ç —Å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ ({len(documents)})"
            )

        if ids is None:
            ids = [str(uuid4()) for _ in documents]

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –¥—É–±–ª–∏–∫–∞—Ç—ã ID
        if len(ids) != len(set(ids)):
            raise ValueError("ID –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å —É–Ω–∏–∫–∞–ª—å–Ω—ã–º–∏")

        # –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è dense embeddings –∏ sparse embeddings
        logger.debug(
            f"üîÑ [retriever][vector_search] –ù–∞—á–∞–ª–æ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ embeddings –¥–ª—è {len(documents)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"
        )

        # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ —á–µ—Ä–µ–∑ asyncio.gather
        dense_embeddings, sparse_embeddings = await asyncio.gather(
            asyncio.to_thread(self.embedding_model.encode_document, documents),
            asyncio.to_thread(self.bm25.encode_documents, documents),
        )

        logger.debug(
            f"‚úÖ [retriever][vector_search] Embeddings —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω—ã: dense={len(dense_embeddings)}, sparse={len(sparse_embeddings)}"
        )

        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–æ—á–µ–∫ –¥–ª—è Qdrant —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
        points = []
        for document_id, document, dense_emb, sparse_emb, metadata in zip(
            ids, documents, dense_embeddings, sparse_embeddings, metadatas, strict=False
        ):
            vectors = {
                "dense": dense_emb,
                "bm25": sparse_emb.as_object(),
            }
            payload = {"text": document, **metadata}

            points.append(PointStruct(id=document_id, vector=vectors, payload=payload))

        await self._ensure_collection()

        async def _upsert_operation():
            return await self.client.upsert(collection_name=self.collection_name, points=points)

        try:
            await retry_with_backoff(
                _upsert_operation,
                max_retries=self.max_retries,
                initial_delay=self.retry_initial_delay,
                max_delay=self.retry_max_delay,
                exponential_base=self.retry_exponential_base,
                jitter=self.retry_jitter,
            )
            logger.info(
                f"‚úÖ [retriever][vector_search] –î–æ–±–∞–≤–ª–µ–Ω–æ {len(documents)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –∫–æ–ª–ª–µ–∫—Ü–∏—é {self.collection_name}"
            )
        except Exception as e:
            error_traceback = traceback.format_exc()
            logger.error(
                f"‚ùå [retriever][vector_search] –û—à–∏–±–∫–∞ –ø—Ä–∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ Qdrant: {type(e).__name__}: {e}\n{error_traceback}",
                exc_info=True,
            )
            raise

    async def search(
        self,
        query: str,
        top_k: int = 10,
        search_type: Literal["dense", "sparse", "hybrid"] = "hybrid",
    ) -> list[tuple[str, float, str, dict | None]]:
        """
        –ü–æ–∏—Å–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ –∑–∞–ø—Ä–æ—Å—É —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ç–∏–ø–æ–≤ –ø–æ–∏—Å–∫–∞

        Args:
            query (str): –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            top_k (int): –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            search_type (Literal["dense", "sparse", "hybrid"]): –¢–∏–ø –ø–æ–∏—Å–∫–∞ (dense, sparse, hybrid). –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é "hybrid"

        Returns:
            list[tuple[str, float, str, dict | None]]: –°–ø–∏—Å–æ–∫ –∫–æ—Ä—Ç–µ–∂–µ–π (ID –¥–æ–∫—É–º–µ–Ω—Ç–∞, score, —Ç–µ–∫—Å—Ç, –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ)

        Raises:
            ValueError: –ï—Å–ª–∏ –∑–∞–ø—Ä–æ—Å –ø—É—Å—Ç –∏–ª–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –Ω–µ–≤–∞–ª–∏–¥–Ω—ã
        """
        if not query or not query.strip():
            logger.warning("‚ö†Ô∏è [retriever][vector_search] –ü–µ—Ä–µ–¥–∞–Ω –ø—É—Å—Ç–æ–π –∑–∞–ø—Ä–æ—Å")
            return []

        if top_k < 1:
            logger.error(f"‚ùå [retriever][vector_search] top_k –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å >= 1, –ø–æ–ª—É—á–µ–Ω–æ: {top_k}")
            return []

        if search_type == "hybrid":
            return await self._hybrid_search(query, top_k, self.prefetch_ratio)
        elif search_type == "dense":
            return await self._dense_search(query, top_k)
        elif search_type == "sparse":
            return await self._sparse_search(query, top_k)

    async def _dense_search(self, query: str, top_k: int) -> list[tuple[str, float, str, dict | None]]:
        """
        –ü–æ–∏—Å–∫ —Ç–æ–ª—å–∫–æ –ø–æ dense –≤–µ–∫—Ç–æ—Ä–∞–º

        Args:
            query (str): –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            top_k (int): –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

        Returns:
            list[tuple[str, float, str, dict | None]]: –°–ø–∏—Å–æ–∫ –∫–æ—Ä—Ç–µ–∂–µ–π (ID –¥–æ–∫—É–º–µ–Ω—Ç–∞, score, —Ç–µ–∫—Å—Ç, –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ)
        """
        logger.debug(f"üîç [retriever][vector_search] –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ dense –ø–æ–∏—Å–∫–∞ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞: {query[:50]}...")
        query_embedding = await asyncio.to_thread(self.embedding_model.encode_query, query)

        async def _search_operation() -> list:
            return await self.client.search(
                collection_name=self.collection_name,
                query_vector=("dense", query_embedding),
                limit=top_k,
                with_payload=True,
            )

        try:
            search_results = await retry_with_backoff(
                _search_operation,
                max_retries=self.max_retries,
                initial_delay=self.retry_initial_delay,
                max_delay=self.retry_max_delay,
                exponential_base=self.retry_exponential_base,
                jitter=self.retry_jitter,
            )
        except Exception as e:
            error_traceback = traceback.format_exc()
            logger.error(
                f"‚ùå [retriever][vector_search] –û—à–∏–±–∫–∞ –ø—Ä–∏ dense –ø–æ–∏—Å–∫–µ: {type(e).__name__}: {e}\n{error_traceback}",
                exc_info=True,
            )
            raise

        results = []
        for result in search_results:
            text = result.payload.get("text", "")
            metadata = {k: v for k, v in result.payload.items() if k != "text"}
            results.append((str(result.id), float(result.score), text, metadata))

        return results

    async def _sparse_search(self, query: str, top_k: int) -> list[tuple[str, float, str, dict | None]]:
        """
        –ü–æ–∏—Å–∫ —Ç–æ–ª—å–∫–æ –ø–æ sparse –≤–µ–∫—Ç–æ—Ä–∞–º

        Args:
            query (str): –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            top_k (int): –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

        Returns:
            list[tuple[str, float, str, dict | None]]: –°–ø–∏—Å–æ–∫ –∫–æ—Ä—Ç–µ–∂–µ–π (ID –¥–æ–∫—É–º–µ–Ω—Ç–∞, score, —Ç–µ–∫—Å—Ç, –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ)
        """
        logger.debug(f"üîç [retriever][vector_search] –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ sparse –ø–æ–∏—Å–∫–∞ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞: {query[:50]}...")
        query_embedding = await asyncio.to_thread(self.bm25.encode_query, query)

        async def _search_operation() -> list:
            return await self.client.search(
                collection_name=self.collection_name,
                query_vector=("bm25", query_embedding),
                limit=top_k,
                with_payload=True,
            )

        try:
            search_results = await retry_with_backoff(
                _search_operation,
                max_retries=self.max_retries,
                initial_delay=self.retry_initial_delay,
                max_delay=self.retry_max_delay,
                exponential_base=self.retry_exponential_base,
                jitter=self.retry_jitter,
            )
        except Exception as e:
            error_traceback = traceback.format_exc()
            logger.error(
                f"‚ùå [retriever][vector_search] –û—à–∏–±–∫–∞ –ø—Ä–∏ sparse –ø–æ–∏—Å–∫–µ: {type(e).__name__}: {e}\n{error_traceback}",
                exc_info=True,
            )
            raise

        results = []
        for result in search_results:
            text = result.payload.get("text", "")
            metadata = {k: v for k, v in result.payload.items() if k != "text"}
            results.append((str(result.id), float(result.score), text, metadata))

        return results

    async def _hybrid_search(
        self,
        query: str,
        top_k: int,
        prefetch_ratio: float,
    ) -> list[tuple[str, float, str, dict | None]]:
        """
        –ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º prefetch –∏ RRF

        Args:
            query (str): –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            top_k (int): –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            prefetch_ratio (float): –í–æ —Å–∫–æ–ª—å–∫–æ —Ä–∞–∑ –±–æ–ª—å—à–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è prefetch

        Returns:
            list[tuple[str, float, str, dict | None]]: –°–ø–∏—Å–æ–∫ –∫–æ—Ä—Ç–µ–∂–µ–π (ID –¥–æ–∫—É–º–µ–Ω—Ç–∞, score, —Ç–µ–∫—Å—Ç, –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ)
        """
        logger.debug(f"üîç [retriever][vector_search] –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –≥–∏–±—Ä–∏–¥–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞: {query[:50]}...")
        # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è query embeddings
        dense_query, sparse_query = await asyncio.gather(
            asyncio.to_thread(self.embedding_model.encode_query, query),
            asyncio.to_thread(self.bm25.encode_query, query),
        )

        prefetch = [
            Prefetch(
                query=dense_query,
                using="dense",
                limit=int(top_k * prefetch_ratio),
            ),
            Prefetch(
                query=sparse_query,
                using="bm25",
                limit=int(top_k * prefetch_ratio),
            ),
        ]

        async def _query_operation():
            return await self.client.query_points(
                collection_name=self.collection_name,
                prefetch=prefetch,
                query=FusionQuery(
                    fusion=Fusion.RRF,
                ),
                with_payload=True,
                limit=top_k,
            )

        try:
            search_results = await retry_with_backoff(
                _query_operation,
                max_retries=self.max_retries,
                initial_delay=self.retry_initial_delay,
                max_delay=self.retry_max_delay,
                exponential_base=self.retry_exponential_base,
                jitter=self.retry_jitter,
            )
        except Exception as e:
            error_traceback = traceback.format_exc()
            logger.error(
                f"‚ùå [retriever][vector_search] –û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–∏–±—Ä–∏–¥–Ω–æ–º –ø–æ–∏—Å–∫–µ: {type(e).__name__}: {e}\n{error_traceback}",
                exc_info=True,
            )
            raise

        results = []
        for result in search_results.points:
            text = result.payload.get("text", "")
            metadata = {k: v for k, v in result.payload.items() if k != "text"}
            results.append((str(result.id), float(result.score), text, metadata))

        return results

    async def get_documents(self, doc_ids: list[str]) -> list[tuple[str, str, dict | None]]:
        """
        –ü–æ–ª—É—á–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç—ã –ø–æ –∏—Ö ID

        Args:
            doc_ids (list[str]): –°–ø–∏—Å–æ–∫ ID –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤

        Returns:
            list[tuple[str, str, dict | None]]: –°–ø–∏—Å–æ–∫ –∫–æ—Ä—Ç–µ–∂–µ–π (doc_id, text, metadata)
        """
        if not doc_ids:
            logger.warning("‚ö†Ô∏è [retriever][vector_search] –ü–µ—Ä–µ–¥–∞–Ω –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫ ID –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
            return []

        async def _retrieve_operation():
            return await self.client.retrieve(
                collection_name=self.collection_name,
                ids=doc_ids,
                with_payload=True,
            )

        try:
            results = await retry_with_backoff(
                _retrieve_operation,
                max_retries=self.max_retries,
                initial_delay=self.retry_initial_delay,
                max_delay=self.retry_max_delay,
                exponential_base=self.retry_exponential_base,
                jitter=self.retry_jitter,
            )

            documents = []
            for point in results:
                text = point.payload.get("text", "")
                metadata = {k: v for k, v in point.payload.items() if k != "text"}
                documents.append((str(point.id), text, metadata if metadata else None))

            logger.info(
                f"‚úÖ [retriever][vector_search] –ü–æ–ª—É—á–µ–Ω–æ {len(documents)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑ {len(doc_ids)} –∑–∞–ø—Ä–æ—à–µ–Ω–Ω—ã—Ö"
            )
            return documents
        except Exception as e:
            error_traceback = traceback.format_exc()
            logger.error(
                f"‚ùå [retriever][vector_search] –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {type(e).__name__}: {e}\n{error_traceback}",
                exc_info=True,
            )
            raise

    async def get_all_documents(self) -> list[tuple[str, str, dict | None]]:
        """
        –ü–æ–ª—É—á–∏—Ç—å –≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏–∑ –∫–æ–ª–ª–µ–∫—Ü–∏–∏

        Returns:
            list[tuple[str, str, dict | None]]: –°–ø–∏—Å–æ–∫ –∫–æ—Ä—Ç–µ–∂–µ–π (doc_id, text, metadata)
        """
        async def _scroll_operation():
            return await self.client.scroll(
                collection_name=self.collection_name,
                limit=None,
                with_payload=True,
            )

        try:
            points, _ = await retry_with_backoff(
                _scroll_operation,
                max_retries=self.max_retries,
                initial_delay=self.retry_initial_delay,
                max_delay=self.retry_max_delay,
                exponential_base=self.retry_exponential_base,
                jitter=self.retry_jitter,
            )

            documents = []
            for point in points:
                text = point.payload.get("text", "")
                metadata = {k: v for k, v in point.payload.items() if k != "text"}
                documents.append((str(point.id), text, metadata if metadata else None))

            logger.info(f"‚úÖ [retriever][vector_search] –ü–æ–ª—É—á–µ–Ω–æ {len(documents)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑ –∫–æ–ª–ª–µ–∫—Ü–∏–∏")
            return documents
        except Exception as e:
            error_traceback = traceback.format_exc()
            logger.error(
                f"‚ùå [retriever][vector_search] –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –≤—Å–µ—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {type(e).__name__}: {e}\n{error_traceback}",
                exc_info=True,
            )
            raise

    async def delete_documents(self, ids: list[str]) -> None:
        """
        –£–¥–∞–ª–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏–∑ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö –ø–æ –∏—Ö ID

        Args:
            ids (list[str]): –°–ø–∏—Å–æ–∫ ID –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è
        """
        if not ids:
            logger.warning("‚ö†Ô∏è [retriever][vector_search] –ü–µ—Ä–µ–¥–∞–Ω –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫ ID –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è")
            return

        logger.info(f"üîÑ [retriever][vector_search] –£–¥–∞–ª–µ–Ω–∏–µ {len(ids)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ {self.collection_name}")
        
        async def _delete_operation():
            return await self.client.delete(
                collection_name=self.collection_name,
                points_selector=PointIdsList(points=ids),
            )

        try:
            await retry_with_backoff(
                _delete_operation,
                max_retries=self.max_retries,
                initial_delay=self.retry_initial_delay,
                max_delay=self.retry_max_delay,
                exponential_base=self.retry_exponential_base,
                jitter=self.retry_jitter,
            )
            logger.info(
                f"‚úÖ [retriever][vector_search] –£—Å–ø–µ—à–Ω–æ —É–¥–∞–ª–µ–Ω–æ {len(ids)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ {self.collection_name}"
            )
        except Exception as e:
            error_traceback = traceback.format_exc()
            logger.error(
                f"‚ùå [retriever][vector_search] –û—à–∏–±–∫–∞ –ø—Ä–∏ —É–¥–∞–ª–µ–Ω–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑ Qdrant: {type(e).__name__}: {e}\n{error_traceback}",
                exc_info=True,
            )
            raise

    async def delete_all_documents(self) -> None:
        """–£–¥–∞–ª–∏—Ç—å –≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏–∑ –∫–æ–ª–ª–µ–∫—Ü–∏–∏"""
        logger.warning("‚ö†Ô∏è [retriever][vector_search] –£–¥–∞–ª–µ–Ω–∏–µ –≤—Å–µ—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑ –∫–æ–ª–ª–µ–∫—Ü–∏–∏")
        
        async def _delete_collection_operation():
            return await self.client.delete_collection(collection_name=self.collection_name)

        try:
            await retry_with_backoff(
                _delete_collection_operation,
                max_retries=self.max_retries,
                initial_delay=self.retry_initial_delay,
                max_delay=self.retry_max_delay,
                exponential_base=self.retry_exponential_base,
                jitter=self.retry_jitter,
            )
            logger.info(f"‚úÖ [retriever][vector_search] –ö–æ–ª–ª–µ–∫—Ü–∏—è {self.collection_name} —É–¥–∞–ª–µ–Ω–∞")
            await self._ensure_collection()
            logger.info(f"‚úÖ [retriever][vector_search] –ö–æ–ª–ª–µ–∫—Ü–∏—è {self.collection_name} –ø–µ—Ä–µ—Å–æ–∑–¥–∞–Ω–∞")
        except Exception as e:
            error_traceback = traceback.format_exc()
            logger.error(
                f"‚ùå [retriever][vector_search] –û—à–∏–±–∫–∞ –ø—Ä–∏ —É–¥–∞–ª–µ–Ω–∏–∏ –≤—Å–µ—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {type(e).__name__}: {e}\n{error_traceback}",
                exc_info=True,
            )
            raise
