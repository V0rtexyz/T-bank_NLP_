import logging
from typing import Literal

import httpx

from tplexity.generation.config import settings
from tplexity.llm_client import get_llm

logger = logging.getLogger(__name__)

SYSTEM_PROMPT = """
Ð¢Ñ‹ - Ð¿Ð¾Ð»ÐµÐ·Ð½Ñ‹Ð¹ AI-Ð°ÑÑÐ¸ÑÑ‚ÐµÐ½Ñ‚. ÐžÑ‚Ð²ÐµÑ‡Ð°Ð¹ Ð½Ð° Ð²Ð¾Ð¿Ñ€Ð¾ÑÑ‹ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ðµ Ð¿Ñ€ÐµÐ´Ð¾ÑÑ‚Ð°Ð²Ð»ÐµÐ½Ð½Ð¾Ð³Ð¾ ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚Ð°.
Ð•ÑÐ»Ð¸ Ð² ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚Ðµ Ð½ÐµÑ‚ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸Ð¸ Ð´Ð»Ñ Ð¾Ñ‚Ð²ÐµÑ‚Ð°, Ñ‡ÐµÑÑ‚Ð½Ð¾ ÑÐºÐ°Ð¶Ð¸ Ð¾Ð± ÑÑ‚Ð¾Ð¼.
"""


class RetrieverClient:
    """ÐšÐ»Ð¸ÐµÐ½Ñ‚ Ð´Ð»Ñ Ð²Ð·Ð°Ð¸Ð¼Ð¾Ð´ÐµÐ¹ÑÑ‚Ð²Ð¸Ñ Ñ Retriever API"""

    def __init__(self, base_url: str, timeout: float = 30.0):
        """
        Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ ÐºÐ»Ð¸ÐµÐ½Ñ‚Ð°

        Args:
            base_url: Ð‘Ð°Ð·Ð¾Ð²Ñ‹Ð¹ URL Retriever API (Ð½Ð°Ð¿Ñ€Ð¸Ð¼ÐµÑ€, http://localhost:8000)
            timeout: Ð¢Ð°Ð¹Ð¼Ð°ÑƒÑ‚ Ð·Ð°Ð¿Ñ€Ð¾ÑÐ¾Ð² Ð² ÑÐµÐºÑƒÐ½Ð´Ð°Ñ…
        """
        self.base_url = base_url.rstrip("/")
        self.timeout = timeout
        logger.info(f"ðŸ”„ [retriever_client] Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½ ÐºÐ»Ð¸ÐµÐ½Ñ‚ Ð´Ð»Ñ {self.base_url}")

    async def search(
        self, query: str, top_k: int | None = None, top_n: int | None = None, use_rerank: bool = True
    ) -> list[tuple[str, float, str, dict | None]]:
        """
        ÐŸÐ¾Ð¸ÑÐº Ñ€ÐµÐ»ÐµÐ²Ð°Ð½Ñ‚Ð½Ñ‹Ñ… Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð¾Ð²

        Args:
            query: ÐŸÐ¾Ð¸ÑÐºÐ¾Ð²Ñ‹Ð¹ Ð·Ð°Ð¿Ñ€Ð¾Ñ
            top_k: ÐšÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð¾Ð² Ð´Ð¾ Ñ€ÐµÑ€Ð°Ð½ÐºÐ°
            top_n: ÐšÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð¾Ð² Ð¿Ð¾ÑÐ»Ðµ Ñ€ÐµÑ€Ð°Ð½ÐºÐ°
            use_rerank: Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÑŒ Ð»Ð¸ reranking

        Returns:
            list[tuple[str, float, str, dict | None]]: Ð¡Ð¿Ð¸ÑÐ¾Ðº ÐºÐ¾Ñ€Ñ‚ÐµÐ¶ÐµÐ¹ (doc_id, score, text, metadata)
        """
        payload = {
            "query": query,
            "use_rerank": use_rerank,
        }

        if top_k is not None:
            payload["top_k"] = top_k
        if top_n is not None:
            payload["top_n"] = top_n

        try:
            async with httpx.AsyncClient(timeout=self.timeout) as client:
                response = await client.post(f"{self.base_url}/retriever/search", json=payload)
                response.raise_for_status()

                data = response.json()
                results = data.get("results", [])

                # ÐŸÑ€ÐµÐ¾Ð±Ñ€Ð°Ð·ÑƒÐµÐ¼ Ð² Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚ (doc_id, score, text, metadata)
                return [(r["doc_id"], r["score"], r["text"], r.get("metadata")) for r in results]

        except httpx.TimeoutException:
            logger.error(f"â±ï¸ [retriever_client] Ð¢Ð°Ð¹Ð¼Ð°ÑƒÑ‚ Ð¿Ñ€Ð¸ Ð·Ð°Ð¿Ñ€Ð¾ÑÐµ Ðº Retriever API")
            raise
        except httpx.HTTPStatusError as e:
            logger.error(f"âŒ [retriever_client] HTTP Ð¾ÑˆÐ¸Ð±ÐºÐ° Ð¾Ñ‚ Retriever API: {e.response.status_code}")
            raise
        except Exception as e:
            logger.error(f"âŒ [retriever_client] ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ Ð·Ð°Ð¿Ñ€Ð¾ÑÐµ Ðº Retriever API: {e}")
            raise


class GenerationService:
    """Ð¡ÐµÑ€Ð²Ð¸Ñ Ð´Ð»Ñ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸ Ð¾Ñ‚Ð²ÐµÑ‚Ð¾Ð² Ñ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸ÐµÐ¼ RAG (Retrieval-Augmented Generation)

    ÐŸÑ€Ð¾Ñ†ÐµÑÑ:
    1. ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÑ‚ Ð·Ð°Ð¿Ñ€Ð¾Ñ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ
    2. Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ RetrieverService Ð´Ð»Ñ Ð¿Ð¾Ð¸ÑÐºÐ° Ñ€ÐµÐ»ÐµÐ²Ð°Ð½Ñ‚Ð½Ñ‹Ñ… Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð¾Ð²
    3. Ð¤Ð¾Ñ€Ð¼Ð¸Ñ€ÑƒÐµÑ‚ Ð¿Ñ€Ð¾Ð¼Ð¿Ñ‚ Ñ ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚Ð¾Ð¼
    4. Ð“ÐµÐ½ÐµÑ€Ð¸Ñ€ÑƒÐµÑ‚ Ð¾Ñ‚Ð²ÐµÑ‚ Ñ‡ÐµÑ€ÐµÐ· LLM
    """

    def __init__(
        self,
        llm_provider: Literal["qwen", "yandexgpt", "chatgpt", "gemini"] | None = None,
        retriever_url: str | None = None,
    ):
        """
        Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ ÑÐµÑ€Ð²Ð¸ÑÐ° Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸

        Args:
            llm_provider (Literal["qwen", "yandexgpt", "chatgpt", "gemini"] | None): ÐŸÑ€Ð¾Ð²Ð°Ð¹Ð´ÐµÑ€ LLM
            retriever_url (str | None): URL Retriever API (ÐµÑÐ»Ð¸ None, Ð±ÐµÑ€ÐµÑ‚ÑÑ Ð¸Ð· config)
        """
        logger.info("ðŸ”„ [generation_service] Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ ÑÐµÑ€Ð²Ð¸ÑÐ° Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸")

        # Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð¸Ñ€ÑƒÐµÐ¼ ÐºÐ»Ð¸ÐµÐ½Ñ‚ Ð´Ð»Ñ Retriever API
        retriever_url = retriever_url or settings.retriever_api_url
        self.retriever_client = RetrieverClient(retriever_url, timeout=settings.retriever_api_timeout)

        # Ð’Ñ‹Ð±Ð¸Ñ€Ð°ÐµÐ¼ Ð¿Ñ€Ð¾Ð²Ð°Ð¹Ð´ÐµÑ€ LLM
        self.llm_provider = llm_provider or settings.llm_provider
        self.llm_client = get_llm(self.llm_provider)

        logger.info(f"âœ… [generation_service] Ð¡ÐµÑ€Ð²Ð¸Ñ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸ Ð¸Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½: provider={self.llm_provider}")

    def _build_prompt(self, query: str, context_documents: list[tuple[str, float, str, dict | None]]) -> str:
        """
        Ð¤Ð¾Ñ€Ð¼Ð¸Ñ€ÑƒÐµÑ‚ Ð¿Ñ€Ð¾Ð¼Ð¿Ñ‚ Ñ ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚Ð¾Ð¼ Ð´Ð»Ñ LLM

        Args:
            query: Ð—Ð°Ð¿Ñ€Ð¾Ñ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ
            context_documents: Ð¡Ð¿Ð¸ÑÐ¾Ðº ÐºÐ¾Ñ€Ñ‚ÐµÐ¶ÐµÐ¹ (doc_id, score, text, metadata)

        Returns:
            str: Ð¡Ñ„Ð¾Ñ€Ð¼Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ð¹ Ð¿Ñ€Ð¾Ð¼Ð¿Ñ‚
        """
        # Ð¤Ð¾Ñ€Ð¼Ð¸Ñ€ÑƒÐµÐ¼ ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚ Ð¸Ð· Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð¾Ð²
        context_parts = []
        for idx, (doc_id, score, text, _metadata) in enumerate(context_documents, 1):
            context_parts.append(f"[Ð”Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚ {idx} (ID: {doc_id}, Ñ€ÐµÐ»ÐµÐ²Ð°Ð½Ñ‚Ð½Ð¾ÑÑ‚ÑŒ: {score:.3f})]\n{text}")

        context = "\n\n".join(context_parts)

        # Ð¤Ð¾Ñ€Ð¼Ð¸Ñ€ÑƒÐµÐ¼ Ñ„Ð¸Ð½Ð°Ð»ÑŒÐ½Ñ‹Ð¹ Ð¿Ñ€Ð¾Ð¼Ð¿Ñ‚
        prompt = f"""ÐšÐ¾Ð½Ñ‚ÐµÐºÑÑ‚:
{context}

Ð’Ð¾Ð¿Ñ€Ð¾Ñ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ: {query}

ÐžÑ‚Ð²ÐµÑ‚ÑŒ Ð½Ð° Ð²Ð¾Ð¿Ñ€Ð¾Ñ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ðµ Ð¿Ñ€ÐµÐ´Ð¾ÑÑ‚Ð°Ð²Ð»ÐµÐ½Ð½Ð¾Ð³Ð¾ ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚Ð°. Ð•ÑÐ»Ð¸ Ð² ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚Ðµ Ð½ÐµÑ‚ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸Ð¸ Ð´Ð»Ñ Ð¾Ñ‚Ð²ÐµÑ‚Ð°, Ñ‡ÐµÑÑ‚Ð½Ð¾ ÑÐºÐ°Ð¶Ð¸ Ð¾Ð± ÑÑ‚Ð¾Ð¼."""

        return prompt

    async def _call_llm(
        self, messages: list[dict[str, str]], temperature: float | None = None, max_tokens: int | None = None
    ) -> str:
        """
        Ð’Ñ‹Ð·Ð¾Ð² LLM Ñ‡ÐµÑ€ÐµÐ· LLMClient

        Args:
            messages: Ð¡Ð¿Ð¸ÑÐ¾Ðº ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ð¹ Ð² Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚Ðµ OpenAI
            temperature: Ð¢ÐµÐ¼Ð¿ÐµÑ€Ð°Ñ‚ÑƒÑ€Ð° Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸ (ÐµÑÐ»Ð¸ None, Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ÑÑ Ð¸Ð· settings.llm)
            max_tokens: ÐœÐ°ÐºÑÐ¸Ð¼Ð°Ð»ÑŒÐ½Ð¾Ðµ ÐºÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ Ñ‚Ð¾ÐºÐµÐ½Ð¾Ð² (ÐµÑÐ»Ð¸ None, Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ÑÑ Ð¸Ð· settings.llm)

        Returns:
            str: Ð¡Ð³ÐµÐ½ÐµÑ€Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ð¹ Ð¾Ñ‚Ð²ÐµÑ‚
        """
        logger.debug("ðŸ”„ [generation_service] ÐžÑ‚Ð¿Ñ€Ð°Ð²ÐºÐ° Ð·Ð°Ð¿Ñ€Ð¾ÑÐ° Ðº LLM")
        return await self.llm_client.generate(messages, temperature=temperature, max_tokens=max_tokens)

    async def generate(
        self,
        query: str,
        top_k: int | None = None,
        use_rerank: bool | None = None,
        temperature: float | None = None,
        max_tokens: int | None = None,
    ) -> tuple[str, list[str], list[dict | None]]:
        """
        Ð“ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ñ Ð¾Ñ‚Ð²ÐµÑ‚Ð° Ñ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸ÐµÐ¼ RAG

        Args:
            query: Ð—Ð°Ð¿Ñ€Ð¾Ñ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ
            top_k: ÐšÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð¾Ð² Ð´Ð»Ñ ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚Ð° (ÐµÑÐ»Ð¸ None, Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ÑÑ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ðµ Ð¸Ð· retriever config)
            use_rerank: Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÑŒ Ð»Ð¸ reranking (ÐµÑÐ»Ð¸ None, Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ÑÑ True Ð¿Ð¾ ÑƒÐ¼Ð¾Ð»Ñ‡Ð°Ð½Ð¸ÑŽ)
            temperature: Ð¢ÐµÐ¼Ð¿ÐµÑ€Ð°Ñ‚ÑƒÑ€Ð° Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸ (ÐµÑÐ»Ð¸ None, Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ÑÑ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ðµ Ð¸Ð· llm config)
            max_tokens: ÐœÐ°ÐºÑÐ¸Ð¼Ð°Ð»ÑŒÐ½Ð¾Ðµ ÐºÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ Ñ‚Ð¾ÐºÐµÐ½Ð¾Ð² (ÐµÑÐ»Ð¸ None, Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ÑÑ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ðµ Ð¸Ð· llm config)

        Returns:
            tuple[str, list[str], list[dict | None]]: (Ð¾Ñ‚Ð²ÐµÑ‚, ÑÐ¿Ð¸ÑÐ¾Ðº doc_ids, ÑÐ¿Ð¸ÑÐ¾Ðº Ð¼ÐµÑ‚Ð°Ð´Ð°Ð½Ð½Ñ‹Ñ…)

        Raises:
            ValueError: Ð•ÑÐ»Ð¸ Ð·Ð°Ð¿Ñ€Ð¾Ñ Ð¿ÑƒÑÑ‚
        """
        if not query or not query.strip():
            raise ValueError("Ð—Ð°Ð¿Ñ€Ð¾Ñ Ð½Ðµ Ð¼Ð¾Ð¶ÐµÑ‚ Ð±Ñ‹Ñ‚ÑŒ Ð¿ÑƒÑÑ‚Ñ‹Ð¼")

        # Ð•ÑÐ»Ð¸ use_rerank Ð½Ðµ ÑƒÐºÐ°Ð·Ð°Ð½, Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ True Ð¿Ð¾ ÑƒÐ¼Ð¾Ð»Ñ‡Ð°Ð½Ð¸ÑŽ
        use_rerank = use_rerank if use_rerank is not None else True

        logger.info(f"ðŸ”„ [generation_service] ÐÐ°Ñ‡Ð°Ð»Ð¾ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸ Ð´Ð»Ñ Ð·Ð°Ð¿Ñ€Ð¾ÑÐ°: {query[:50]}...")

        # Ð¨Ð°Ð³ 1: ÐŸÐ¾Ð¸ÑÐº Ñ€ÐµÐ»ÐµÐ²Ð°Ð½Ñ‚Ð½Ñ‹Ñ… Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð¾Ð² Ñ‡ÐµÑ€ÐµÐ· Retriever API
        logger.debug(f"ðŸ” [generation_service] ÐŸÐ¾Ð¸ÑÐº Ñ€ÐµÐ»ÐµÐ²Ð°Ð½Ñ‚Ð½Ñ‹Ñ… Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð¾Ð², top_k={top_k}, use_rerank={use_rerank}")
        context_documents = await self.retriever_client.search(
            query=query, top_k=top_k, top_n=top_k, use_rerank=use_rerank
        )

        if not context_documents:
            logger.warning("âš ï¸ [generation_service] ÐÐµ Ð½Ð°Ð¹Ð´ÐµÐ½Ð¾ Ñ€ÐµÐ»ÐµÐ²Ð°Ð½Ñ‚Ð½Ñ‹Ñ… Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð¾Ð²")
            return (
                "Ðš ÑÐ¾Ð¶Ð°Ð»ÐµÐ½Ð¸ÑŽ, Ñ Ð½Ðµ Ð½Ð°ÑˆÐµÐ» Ñ€ÐµÐ»ÐµÐ²Ð°Ð½Ñ‚Ð½Ð¾Ð¹ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸Ð¸ Ð² Ð±Ð°Ð·Ðµ Ð·Ð½Ð°Ð½Ð¸Ð¹ Ð´Ð»Ñ Ð¾Ñ‚Ð²ÐµÑ‚Ð° Ð½Ð° Ð²Ð°Ñˆ Ð²Ð¾Ð¿Ñ€Ð¾Ñ.",
                [],
                [],
            )

        logger.info(f"âœ… [generation_service] ÐÐ°Ð¹Ð´ÐµÐ½Ð¾ {len(context_documents)} Ñ€ÐµÐ»ÐµÐ²Ð°Ð½Ñ‚Ð½Ñ‹Ñ… Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð¾Ð²")

        # Ð¨Ð°Ð³ 2: Ð¤Ð¾Ñ€Ð¼Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð¿Ñ€Ð¾Ð¼Ð¿Ñ‚Ð°
        logger.debug("ðŸ”„ [generation_service] Ð¤Ð¾Ñ€Ð¼Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð¿Ñ€Ð¾Ð¼Ð¿Ñ‚Ð° Ñ ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚Ð¾Ð¼")
        prompt = self._build_prompt(query, context_documents)

        # Ð¨Ð°Ð³ 3: Ð“ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ñ Ð¾Ñ‚Ð²ÐµÑ‚Ð° Ñ‡ÐµÑ€ÐµÐ· LLM
        messages = [
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user", "content": prompt},
        ]

        logger.debug("ðŸ”„ [generation_service] Ð“ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ñ Ð¾Ñ‚Ð²ÐµÑ‚Ð° Ñ‡ÐµÑ€ÐµÐ· LLM")
        answer = await self._call_llm(messages, temperature=temperature, max_tokens=max_tokens)
        logger.info("âœ… [generation_service] ÐžÑ‚Ð²ÐµÑ‚ ÑƒÑÐ¿ÐµÑˆÐ½Ð¾ ÑÐ³ÐµÐ½ÐµÑ€Ð¸Ñ€Ð¾Ð²Ð°Ð½")

        # Ð˜Ð·Ð²Ð»ÐµÐºÐ°ÐµÐ¼ Ð¸ÑÑ‚Ð¾Ñ‡Ð½Ð¸ÐºÐ¸ (Ð²ÑÐµÐ³Ð´Ð° Ð²ÐºÐ»ÑŽÑ‡Ð°ÐµÐ¼)
        doc_ids = [doc_id for doc_id, _, _, _ in context_documents]
        metadatas = [metadata for _, _, _, metadata in context_documents]

        return answer, doc_ids, metadatas

    async def close(self) -> None:
        """Ð—Ð°ÐºÑ€Ñ‹Ñ‚Ð¸Ðµ LLM ÐºÐ»Ð¸ÐµÐ½Ñ‚Ð°"""
        if hasattr(self, "llm_client") and hasattr(self.llm_client, "client"):
            await self.llm_client.client.close()
