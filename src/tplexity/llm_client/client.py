import json
import logging

from openai import AsyncOpenAI

from tplexity.llm_client.config import settings

logger = logging.getLogger(__name__)


# Singleton –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞
_llm_instances: dict[str, "LLMClient"] = {}


class LLMClient:
    """–ö–ª–∏–µ–Ω—Ç –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å LLM —á–µ—Ä–µ–∑ OpenAI-—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π API"""

    def __init__(
        self,
        model: str,
        api_key: str,
        base_url: str | None = None,
        timeout: int = 60,
        **kwargs,
    ):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è LLM –∫–ª–∏–µ–Ω—Ç–∞

        Args:
            model: –ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
            api_key: API –∫–ª—é—á
            base_url: –ë–∞–∑–æ–≤—ã–π URL –¥–ª—è API (–µ—Å–ª–∏ None, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π OpenAI API)
            timeout: –¢–∞–π–º–∞—É—Ç –¥–ª—è –∑–∞–ø—Ä–æ—Å–æ–≤ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
            **kwargs: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è AsyncOpenAI (–Ω–∞–ø—Ä–∏–º–µ—Ä, default_headers={"x-folder-id": "..."})
        """
        self.model = model
        self.api_key = api_key
        self.base_url = base_url
        self.timeout = timeout

        logger.info(f"üîÑ [llm_client] –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è LLM –∫–ª–∏–µ–Ω—Ç–∞: model={model}, base_url={base_url}")

        self.client = AsyncOpenAI(
            base_url=self.base_url,
            api_key=self.api_key,
            timeout=self.timeout,
            **kwargs,
        )

        logger.info("‚úÖ [llm_client] LLM –∫–ª–∏–µ–Ω—Ç –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")

    async def generate(
        self,
        messages: list[dict[str, str]],
        temperature: float | None = None,
        max_tokens: int | None = None,
        deterministic: bool = False,
    ) -> str:
        """
        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —á–µ—Ä–µ–∑ LLM

        Args:
            messages (list[dict[str, str]]): –°–ø–∏—Å–æ–∫ —Å–æ–æ–±—â–µ–Ω–∏–π –≤ —Ñ–æ—Ä–º–∞—Ç–µ OpenAI
                –ü—Ä–∏–º–µ—Ä: [
                    {"role": "system", "content": "–¢—ã - –ø–æ–º–æ—â–Ω–∏–∫"},
                    {"role": "user", "content": "–ü—Ä–∏–≤–µ—Ç!"}
                ]
            temperature (float | None): –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ (–µ—Å–ª–∏ None, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∏–∑ settings.llm.temperature)
            max_tokens (int | None): –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ (–µ—Å–ª–∏ None, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∏–∑ settings.llm.max_tokens)
            deterministic (bool): –ï—Å–ª–∏ True, –¥–æ–±–∞–≤–ª—è–µ—Ç seed –∏ top_p=1.0 –¥–ª—è –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é False)

        Returns:
            str: –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç

        Raises:
            Exception: –ü—Ä–∏ –æ—à–∏–±–∫–µ –≤—ã–∑–æ–≤–∞ LLM API
        """
        temperature = temperature if temperature is not None else settings.temperature
        max_tokens = max_tokens if max_tokens is not None else settings.max_tokens

        if deterministic:
            logger.info(
                f"üîÑ [llm_client] –û—Ç–ø—Ä–∞–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –∫ LLM (–¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ä–µ–∂–∏–º): "
                f"model={self.model}, base_url={self.base_url}, temperature={temperature}, "
                f"max_tokens={max_tokens}, seed=42, top_p=1.0, do_sample=False"
            )
        else:
            logger.info(
                f"üîÑ [llm_client] –û—Ç–ø—Ä–∞–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –∫ LLM: "
                f"model={self.model}, base_url={self.base_url}, temperature={temperature}, max_tokens={max_tokens}"
            )

        try:
            # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–ø—Ä–æ—Å–∞
            request_kwargs = {
                "model": self.model,
                "messages": messages,
                "temperature": temperature,
                "max_tokens": max_tokens,
            }
            
            # –ï—Å–ª–∏ –∑–∞–ø—Ä–æ—à–µ–Ω–∞ –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è, –¥–æ–±–∞–≤–ª—è–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏
            # 1. seed - —Ñ–∏–∫—Å–∏—Ä—É–µ—Ç –Ω–∞—á–∞–ª—å–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞ —Å–ª—É—á–∞–π–Ω—ã—Ö —á–∏—Å–µ–ª
            # 2. top_p=1.0 - –æ—Ç–∫–ª—é—á–∞–µ—Ç nucleus sampling, –∏—Å–ø–æ–ª—å–∑—É—è –≤—Å–µ —Ç–æ–∫–µ–Ω—ã
            # 3. –ü–µ—Ä–µ–¥–∞–µ–º do_sample=False —á–µ—Ä–µ–∑ extra_body (–µ—Å–ª–∏ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è TGI)
            # –≠—Ç–æ –æ—Å–æ–±–µ–Ω–Ω–æ –≤–∞–∂–Ω–æ –¥–ª—è HuggingFace TGI, –∫–æ—Ç–æ—Ä—ã–π –º–æ–∂–µ—Ç —Ç—Ä–µ–±–æ–≤–∞—Ç—å —ç—Ç–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
            # –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –¥–ª—è –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏ –∑–∞–ø—Ä–æ—Å–æ–≤, –≥–¥–µ –Ω—É–∂–Ω–∞ –ø–æ–ª–Ω–∞—è –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ—Å—Ç—å
            if deterministic:
                request_kwargs["seed"] = 42
                request_kwargs["top_p"] = 1.0  # –û—Ç–∫–ª—é—á–∞–µ—Ç nucleus sampling –¥–ª—è –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏
                # extra_body –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø–µ—Ä–µ–¥–∞—Ç—å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã, —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –¥–ª—è TGI
                # –ü–µ—Ä–µ–¥–∞–µ–º do_sample=False –¥–ª—è –ø–æ–ª–Ω–æ–π –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏
                request_kwargs["extra_body"] = {"do_sample": False}
            
            response = await self.client.chat.completions.create(**request_kwargs)
            

            answer = response.choices[0].message.content

            logger.info(f"‚úÖ [llm_client] –û—Ç–≤–µ—Ç –ø–æ–ª—É—á–µ–Ω –æ—Ç LLM (model={self.model}), –¥–ª–∏–Ω–∞ –æ—Ç–≤–µ—Ç–∞: {len(answer) if answer else 0} —Å–∏–º–≤–æ–ª–æ–≤")
            return answer
        except Exception as e:
            logger.error(f"‚ùå [llm_client] –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–∑–æ–≤–µ LLM: {e}")
            raise


def get_llm(provider: str) -> LLMClient:
    """
    –ü–æ–ª—É—á–∏—Ç—å LLM –∫–ª–∏–µ–Ω—Ç –¥–ª—è —É–∫–∞–∑–∞–Ω–Ω–æ–≥–æ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞ (singleton)

    Args:
        provider (str): –ü—Ä–æ–≤–∞–π–¥–µ—Ä LLM

    Returns:
        LLMClient: –≠–∫–∑–µ–º–ø–ª—è—Ä LLM –∫–ª–∏–µ–Ω—Ç–∞ –¥–ª—è —É–∫–∞–∑–∞–Ω–Ω–æ–≥–æ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞
    """
    global _llm_instances

    if provider in _llm_instances:
        return _llm_instances[provider]

    if provider == "qwen":
        client = LLMClient(
            model=settings.qwen_model,
            api_key=settings.qwen_api_key,
            base_url=settings.qwen_base_url,
            timeout=settings.timeout,
        )
    elif provider == "yandexgpt":
        model_name = f"gpt://{settings.yandexgpt_folder_id}/{settings.yandexgpt_model}"
        client = LLMClient(
            model=model_name,
            api_key=settings.yandexgpt_api_key,
            base_url=settings.yandexgpt_base_url,
            timeout=settings.timeout,
            default_headers={"x-folder-id": settings.yandexgpt_folder_id},
        )
    elif provider == "chatgpt":
        client = LLMClient(
            model=settings.chatgpt_model,
            api_key=settings.chatgpt_api_key,
            base_url=None,
            timeout=settings.timeout,
        )
    elif provider == "deepseek":
        client = LLMClient(
            model=settings.deepseek_model,
            api_key=settings.deepseek_api_key,
            base_url=settings.deepseek_base_url,
            timeout=settings.timeout,
        )
    else:
        raise ValueError(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –ø—Ä–æ–≤–∞–π–¥–µ—Ä LLM: {provider}")

    _llm_instances[provider] = client
    return client
