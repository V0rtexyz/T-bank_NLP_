import logging

from openai import AsyncOpenAI

from tplexity.llm_client.config import settings

logger = logging.getLogger(__name__)


# Singleton –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞
_llm_instances: dict[str, "LLMClient"] = {}


class LLMClient:
    """–ö–ª–∏–µ–Ω—Ç –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å LLM —á–µ—Ä–µ–∑ OpenAI-—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π API"""

    def __init__(
        self,
        model: str,
        api_key: str,
        base_url: str | None = None,
        timeout: int = 60,
        **kwargs,
    ):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è LLM –∫–ª–∏–µ–Ω—Ç–∞

        Args:
            model: –ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
            api_key: API –∫–ª—é—á
            base_url: –ë–∞–∑–æ–≤—ã–π URL –¥–ª—è API (–µ—Å–ª–∏ None, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π OpenAI API)
            timeout: –¢–∞–π–º–∞—É—Ç –¥–ª—è –∑–∞–ø—Ä–æ—Å–æ–≤ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
            **kwargs: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è AsyncOpenAI (–Ω–∞–ø—Ä–∏–º–µ—Ä, default_headers={"x-folder-id": "..."})
        """
        self.model = model
        self.api_key = api_key
        self.base_url = base_url
        self.timeout = timeout

        logger.info(f"üîÑ [llm_client] –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è LLM –∫–ª–∏–µ–Ω—Ç–∞: model={model}, base_url={base_url}")

        self.client = AsyncOpenAI(
            base_url=self.base_url,
            api_key=self.api_key,
            timeout=self.timeout,
            **kwargs,
        )

        logger.info("‚úÖ [llm_client] LLM –∫–ª–∏–µ–Ω—Ç –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")

    async def generate(
        self,
        messages: list[dict[str, str]],
        temperature: float | None = None,
        max_tokens: int | None = None,
    ) -> str:
        """
        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —á–µ—Ä–µ–∑ LLM

        Args:
            messages (list[dict[str, str]]): –°–ø–∏—Å–æ–∫ —Å–æ–æ–±—â–µ–Ω–∏–π –≤ —Ñ–æ—Ä–º–∞—Ç–µ OpenAI
                –ü—Ä–∏–º–µ—Ä: [
                    {"role": "system", "content": "–¢—ã - –ø–æ–º–æ—â–Ω–∏–∫"},
                    {"role": "user", "content": "–ü—Ä–∏–≤–µ—Ç!"}
                ]
            temperature (float | None): –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ (–µ—Å–ª–∏ None, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∏–∑ settings.llm.temperature)
            max_tokens (int | None): –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ (–µ—Å–ª–∏ None, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∏–∑ settings.llm.max_tokens)

        Returns:
            str: –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç

        Raises:
            Exception: –ü—Ä–∏ –æ—à–∏–±–∫–µ –≤—ã–∑–æ–≤–∞ LLM API
        """
        temperature = temperature or settings.temperature
        max_tokens = max_tokens or settings.max_tokens

        logger.info(f"üîÑ [llm_client] –û—Ç–ø—Ä–∞–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –∫ LLM: model={self.model}, base_url={self.base_url}")

        try:
            response = await self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=temperature,
                max_tokens=max_tokens,
            )

            answer = response.choices[0].message.content

            logger.info(f"‚úÖ [llm_client] –û—Ç–≤–µ—Ç –ø–æ–ª—É—á–µ–Ω –æ—Ç LLM (model={self.model})")
            return answer
        except Exception as e:
            logger.error(f"‚ùå [llm_client] –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–∑–æ–≤–µ LLM: {e}")
            raise


def get_llm(provider: str) -> LLMClient:
    """
    –ü–æ–ª—É—á–∏—Ç—å LLM –∫–ª–∏–µ–Ω—Ç –¥–ª—è —É–∫–∞–∑–∞–Ω–Ω–æ–≥–æ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞ (singleton)

    Args:
        provider (str): –ü—Ä–æ–≤–∞–π–¥–µ—Ä LLM

    Returns:
        LLMClient: –≠–∫–∑–µ–º–ø–ª—è—Ä LLM –∫–ª–∏–µ–Ω—Ç–∞ –¥–ª—è —É–∫–∞–∑–∞–Ω–Ω–æ–≥–æ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞
    """
    global _llm_instances

    if provider in _llm_instances:
        return _llm_instances[provider]

    if provider == "qwen":
        client = LLMClient(
            model=settings.qwen_model,
            api_key=settings.qwen_api_key,
            base_url=settings.qwen_base_url,
            timeout=settings.timeout,
        )
    elif provider == "yandexgpt":
        model_name = f"gpt://{settings.yandexgpt_folder_id}/{settings.yandexgpt_model}"
        client = LLMClient(
            model=model_name,
            api_key=settings.yandexgpt_api_key,
            base_url=settings.yandexgpt_base_url,
            timeout=settings.timeout,
            default_headers={"x-folder-id": settings.yandexgpt_folder_id},
        )
    elif provider == "chatgpt":
        client = LLMClient(
            model=settings.chatgpt_model,
            api_key=settings.chatgpt_api_key,
            base_url=None,
            timeout=settings.timeout,
        )
    elif provider == "deepseek":
        client = LLMClient(
            model=settings.deepseek_model,
            api_key=settings.deepseek_api_key,
            base_url=settings.deepseek_base_url,
            timeout=settings.timeout,
        )
    else:
        raise ValueError(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –ø—Ä–æ–≤–∞–π–¥–µ—Ä LLM: {provider}")

    _llm_instances[provider] = client
    return client
