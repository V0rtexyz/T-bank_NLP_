"""
–ú–æ–¥—É–ª—å –¥–ª—è —Ä–∞–∑–±–∏–µ–Ω–∏—è –ø–æ—Å—Ç–æ–≤ –∏–∑ Telegram –Ω–∞ —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ —á–∞–Ω–∫–∏.
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç LangChain RecursiveCharacterTextSplitter –¥–ª—è —É–º–Ω–æ–≥–æ —Ä–∞–∑–±–∏–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞.
"""

import json
import re
from pathlib import Path
from typing import Any

from langchain_text_splitters import RecursiveCharacterTextSplitter


class PostChunker:
    """
    –ö–ª–∞—Å—Å –¥–ª—è —Ä–∞–∑–±–∏–µ–Ω–∏—è –ø–æ—Å—Ç–æ–≤ –Ω–∞ —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ —á–∞–Ω–∫–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º LangChain.
    """

    # –û—Å–Ω–æ–≤–Ω—ã–µ –º–∞—Ä–∫–µ—Ä—ã, –∫–æ—Ç–æ—Ä—ã–µ –æ–±—ã—á–Ω–æ —Ä–∞–∑–¥–µ–ª—è—é—Ç —Ä–∞–∑–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏
    MAIN_SEPARATORS = [
        "\n\nüîπ ",  # –û—Å–Ω–æ–≤–Ω–æ–π –º–∞—Ä–∫–µ—Ä –¥–ª—è –Ω–æ–≤–æ—Å—Ç–µ–π
        "\n\nüü¢ ",  # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –º–∞—Ä–∫–µ—Ä
        "\n\nüî¥ ",  # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –º–∞—Ä–∫–µ—Ä
        "\n\n",  # –î–≤–æ–π–Ω–æ–π –ø–µ—Ä–µ–Ω–æ—Å —Å—Ç—Ä–æ–∫–∏
    ]

    def __init__(self, source_name: str = None, chunk_size: int = 1000, chunk_overlap: int = 100):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —á–∞–Ω–∫–µ—Ä–∞.

        Args:
            source_name: –ù–∞–∑–≤–∞–Ω–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ (–¥–ª—è —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏)
            chunk_size: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞ –≤ —Å–∏–º–≤–æ–ª–∞—Ö
            chunk_overlap: –ü–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ –º–µ–∂–¥—É —á–∞–Ω–∫–∞–º–∏
        """
        self.source_name = source_name
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap

        # –°–æ–∑–¥–∞–µ–º text_splitter —Å –Ω–∞—Å—Ç—Ä–æ–µ–Ω–Ω—ã–º–∏ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è–º–∏
        self.text_splitter = RecursiveCharacterTextSplitter(
            separators=self.MAIN_SEPARATORS + ["\n", " ", ""],
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            length_function=len,
            is_separator_regex=False,
        )

    def chunk_post(self, post: dict[str, Any]) -> list[dict[str, Any]]:
        """
        –†–∞–∑–±–∏–≤–∞–µ—Ç –ø–æ—Å—Ç –Ω–∞ —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ —á–∞–Ω–∫–∏.

        Args:
            post: –°–ª–æ–≤–∞—Ä—å —Å –¥–∞–Ω–Ω—ã–º–∏ –ø–æ—Å—Ç–∞

        Returns:
            –°–ø–∏—Å–æ–∫ —á–∞–Ω–∫–æ–≤ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
        """
        text = post.get("text", "").strip()

        # –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç –ø—É—Å—Ç–æ–π –∏–ª–∏ –æ—á–µ–Ω—å –∫–æ—Ä–æ—Ç–∫–∏–π, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
        if not text or len(text) < 50:
            return []

        # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞
        cleaned_text = self._preprocess_text(text)

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω—É–∂–Ω–æ –ª–∏ —Ä–∞–∑–±–∏–≤–∞—Ç—å –ø–æ—Å—Ç
        if self._is_single_topic(cleaned_text):
            # –ü–æ—Å—Ç –æ–± –æ–¥–Ω–æ–π —Ç–µ–º–µ - –Ω–µ —Ä–∞–∑–±–∏–≤–∞–µ–º
            chunks_texts = [cleaned_text]
        else:
            # –†–∞–∑–±–∏–≤–∞–µ–º —Å –ø–æ–º–æ—â—å—é LangChain
            chunks_texts = self.text_splitter.split_text(cleaned_text)

        # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∫ –∫–∞–∂–¥–æ–º—É —á–∞–Ω–∫—É
        result = []
        chunk_idx = 0

        for chunk_text in chunks_texts:
            chunk_text = chunk_text.strip()

            # –§–∏–ª—å—Ç—Ä—É–µ–º —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —á–∞–Ω–∫–∏
            if len(chunk_text) < 50:
                continue

            # –ü–æ—Å—Ç-–æ–±—Ä–∞–±–æ—Ç–∫–∞ —á–∞–Ω–∫–∞
            chunk_text = self._postprocess_chunk(chunk_text)

            if len(chunk_text) < 50:
                continue

            chunk = {
                "original_id": post.get("id"),
                "original_link": post.get("link"),
                "original_date": post.get("date"),
                "chunk_index": chunk_idx,
                "chunk_text": chunk_text,
                "chunk_length": len(chunk_text),
                "views": post.get("views"),
                "forwards": post.get("forwards"),
                "has_media": post.get("has_media"),
                "media_type": post.get("media_type"),
            }
            result.append(chunk)
            chunk_idx += 1

        return result

    def _preprocess_text(self, text: str) -> str:
        """–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –ø–µ—Ä–µ–¥ —Ä–∞–∑–±–∏–µ–Ω–∏–µ–º."""
        # –£–±–∏—Ä–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏ —Ç–∏–ø–∞ "–î–æ–±—Ä–æ–µ —É—Ç—Ä–æ", "–ò—Ç–æ–≥–∏ –¥–Ω—è"
        headers = [
            r"\*\*‚è∞\*\*\*\* –î–æ–±—Ä–æ–µ —É—Ç—Ä–æ:.*?\*\*\n*",
            r"\*\*–î–æ–±—Ä–æ–µ —É—Ç—Ä–æ:.*?\*\*\n*",
            r"üèÅ\*\* –ò—Ç–æ–≥–∏ –¥–Ω—è:.*?\*\*\n*",
            r"\*\*üí°\*\*\*\*.*?\*\*\n*",
            r"\*\*‚òï\*\*\*\* –ú—ã—Å–ª–∏ —Å —É—Ç—Ä–∞.*?\n*",
        ]

        for pattern in headers:
            text = re.sub(pattern, "", text, flags=re.IGNORECASE)

        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –ø–µ—Ä–µ–Ω–æ—Å—ã —Å—Ç—Ä–æ–∫
        text = re.sub(r"\n{3,}", "\n\n", text)

        return text.strip()

    def _postprocess_chunk(self, chunk: str) -> str:
        """–ü–æ—Å—Ç-–æ–±—Ä–∞–±–æ—Ç–∫–∞ —á–∞–Ω–∫–∞ –ø–æ—Å–ª–µ —Ä–∞–∑–±–∏–µ–Ω–∏—è."""
        # –£–±–∏—Ä–∞–µ–º —Ö–µ—à—Ç–µ–≥–∏ –≤ –∫–æ–Ω—Ü–µ
        chunk = re.sub(r"\n*#\w+\s*$", "", chunk)

        # –£–±–∏—Ä–∞–µ–º —Ç–∏–∫–µ—Ä—ã –∞–∫—Ü–∏–π –≤ –∫–æ–Ω—Ü–µ
        chunk = re.sub(r"\n*\$[A-Z]+(?:\s+\$[A-Z]+)*\s*$", "", chunk)

        # –£–±–∏—Ä–∞–µ–º —É–ø–æ–º–∏–Ω–∞–Ω–∏—è –∫–∞–Ω–∞–ª–æ–≤ –≤ –∫–æ–Ω—Ü–µ
        chunk = re.sub(r"\n*@\w+\s*$", "", chunk)

        # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –∏ –ø–µ—Ä–µ–Ω–æ—Å—ã
        chunk = re.sub(r"\n{3,}", "\n\n", chunk)
        chunk = re.sub(r" {2,}", " ", chunk)

        return chunk.strip()

    def _is_single_topic(self, text: str) -> bool:
        """
        –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –ø–æ—Å—Ç –æ–¥–Ω–æ–π —Ç–µ–º–æ–π —Å –¥–µ—Ç–∞–ª—è–º–∏.

        Returns:
            True –µ—Å–ª–∏ —ç—Ç–æ –æ–¥–Ω–∞ —Ç–µ–º–∞, False –µ—Å–ª–∏ –¥–∞–π–¥–∂–µ—Å—Ç
        """
        # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ –º–∞—Ä–∫–µ—Ä—ã
        main_markers = ["üîπ", "üü¢", "üî¥"]
        nested_markers = ["üü°", "üîµ", "‚ö™Ô∏è", "‚Ä¢", "‚ñ™Ô∏è"]

        main_count = sum(text.count(marker) for marker in main_markers)
        nested_count = sum(text.count(marker) for marker in nested_markers)

        # –ï—Å–ª–∏ —Ç–æ–ª—å–∫–æ –≤–ª–æ–∂–µ–Ω–Ω—ã–µ –º–∞—Ä–∫–µ—Ä—ã (2-8) –∏ –µ—Å—Ç—å –≤–≤–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç
        if nested_count > 0 and main_count == 0:
            marker_positions = [text.find(m) for m in nested_markers if m in text]
            first_marker_pos = min(marker_positions) if marker_positions else -1

            # –ï—Å–ª–∏ –µ—Å—Ç—å –≤–≤–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç –ø–µ—Ä–µ–¥ –º–∞—Ä–∫–µ—Ä–∞–º–∏
            if 2 <= nested_count <= 8 and first_marker_pos > 50 and len(text) < 2500:
                return True

        # –ï—Å–ª–∏ –æ–¥–∏–Ω –æ—Å–Ω–æ–≤–Ω–æ–π –º–∞—Ä–∫–µ—Ä –∏ –µ—Å—Ç—å –≤–ª–æ–∂–µ–Ω–Ω—ã–µ - —ç—Ç–æ –æ–¥–Ω–∞ —Ç–µ–º–∞
        if main_count == 1 and nested_count > 0:
            return True

        # –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç –∫–æ—Ä–æ—Ç–∫–∏–π –∏ –Ω–µ—Ç –æ—Å–Ω–æ–≤–Ω—ã—Ö –º–∞—Ä–∫–µ—Ä–æ–≤ - –æ–¥–Ω–∞ —Ç–µ–º–∞
        if len(text) < 800 and main_count == 0:
            return True

        return False


def process_channel(channel_path: Path, output_path: Path = None):
    """
    –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∫–∞–Ω–∞–ª –∏ —Å–æ–∑–¥–∞–µ—Ç —Ñ–∞–π–ª —Å —á–∞–Ω–∫–∞–º–∏.

    Args:
        channel_path: –ü—É—Ç—å –∫ –ø–∞–ø–∫–µ –∫–∞–Ω–∞–ª–∞
        output_path: –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ñ–∞–π–ª–∞ —Å —á–∞–Ω–∫–∞–º–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
    """
    source_name = channel_path.name
    messages_file = channel_path / "messages_monitor.json"

    if not messages_file.exists():
        print(f"–§–∞–π–ª {messages_file} –Ω–µ –Ω–∞–π–¥–µ–Ω")
        return

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–æ—Å—Ç—ã
    with open(messages_file, encoding="utf-8") as f:
        posts = json.load(f)

    print(f"\n–û–±—Ä–∞–±–æ—Ç–∫–∞ {source_name}: {len(posts)} –ø–æ—Å—Ç–æ–≤")

    # –°–æ–∑–¥–∞–µ–º —á–∞–Ω–∫–µ—Ä
    chunker = PostChunker(source_name=source_name)

    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—ã–π –ø–æ—Å—Ç
    all_chunks = []
    total_posts_with_chunks = 0

    for post in posts:
        chunks = chunker.chunk_post(post)
        if chunks:
            all_chunks.extend(chunks)
            total_posts_with_chunks += 1

    print(f"  –ü–æ—Å—Ç–æ–≤ —Å —Ç–µ–∫—Å—Ç–æ–º: {total_posts_with_chunks}")
    print(f"  –í—Å–µ–≥–æ —á–∞–Ω–∫–æ–≤: {len(all_chunks)}")

    if all_chunks:
        avg_chunks_per_post = len(all_chunks) / total_posts_with_chunks
        print(f"  –°—Ä–µ–¥–Ω–µ–µ —á–∞–Ω–∫–æ–≤ –Ω–∞ –ø–æ—Å—Ç: {avg_chunks_per_post:.2f}")

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
    if output_path is None:
        output_path = channel_path / "messages_chunked.json"

    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(all_chunks, f, ensure_ascii=False, indent=2)

    print(f"  –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ {output_path}")

    return all_chunks


def process_all_channels(data_dir: Path):
    """
    –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≤—Å–µ –∫–∞–Ω–∞–ª—ã –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ data/telegram.

    Args:
        data_dir: –ü—É—Ç—å –∫ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ data
    """
    telegram_dir = data_dir / "telegram"

    if not telegram_dir.exists():
        print(f"–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è {telegram_dir} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
        return

    # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –ø–æ–¥–¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
    channels = [d for d in telegram_dir.iterdir() if d.is_dir()]

    print(f"–ù–∞–π–¥–µ–Ω–æ –∫–∞–Ω–∞–ª–æ–≤: {len(channels)}")
    print("=" * 60)

    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—ã–π –∫–∞–Ω–∞–ª
    for channel_path in channels:
        try:
            process_channel(channel_path)
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {channel_path.name}: {e}")

    print("\n" + "=" * 60)
    print("–û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")


if __name__ == "__main__":
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—É—Ç—å –∫ –¥–∞–Ω–Ω—ã–º
    data_dir = Path(__file__).parent.parent.parent.parent / "data"

    print("–ó–∞–ø—É—Å–∫ —á–∞–Ω–∫–∏—Ä–æ–≤–∞–Ω–∏—è –ø–æ—Å—Ç–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º LangChain...")
    print(f"–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å –¥–∞–Ω–Ω—ã–º–∏: {data_dir}")

    process_all_channels(data_dir)
