"""
–ú–æ–¥—É–ª—å –¥–ª—è –±–∞—Ç—á–∏–Ω–≥–∞ LLM-–∑–∞–ø—Ä–æ—Å–æ–≤ —Å –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–π –æ—á–µ—Ä–µ–¥—å—é.

–ü–æ–∑–≤–æ–ª—è–µ—Ç –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∞—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –∑–∞–ø—Ä–æ—Å–æ–≤ –≤ –æ–¥–∏–Ω –±–∞—Ç—á –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏.
"""

import asyncio
import hashlib
import logging
from dataclasses import dataclass, field
from typing import Any

from tplexity.llm_client import get_llm

logger = logging.getLogger(__name__)


@dataclass
class LLMRequest:
    """–ó–∞–ø—Ä–æ—Å –∫ LLM —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–º"""

    post_text: str
    llm_provider: str
    future: asyncio.Future = field(default_factory=asyncio.Future)
    cache_key: str = field(default="")

    def __post_init__(self):
        """–í—ã—á–∏—Å–ª—è–µ–º cache_key –ø–æ—Å–ª–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏"""
        if not self.cache_key:
            # –°–æ–∑–¥–∞–µ–º –∫–ª—é—á –∫—ç—à–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—Å—Ç–∞ –∏ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞
            text_hash = hashlib.md5(self.post_text.encode()).hexdigest()
            self.cache_key = f"{self.llm_provider}:{text_hash}"


class LLMBatcher:
    """
    –ë–∞—Ç—á–µ—Ä –¥–ª—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏ LLM-–∑–∞–ø—Ä–æ—Å–æ–≤.

    –°–æ–±–∏—Ä–∞–µ—Ç –∑–∞–ø—Ä–æ—Å—ã –≤ –±–∞—Ç—á–∏ –∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∏—Ö –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ.
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤.
    """

    def __init__(
        self,
        batch_size: int = 5,
        batch_timeout: float = 0.5,
        max_cache_size: int = 1000,
        llm_provider: str = "qwen",
    ):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–∞—Ç—á–µ—Ä–∞

        Args:
            batch_size: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞
            batch_timeout: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –≤—Ä–µ–º—è –æ–∂–∏–¥–∞–Ω–∏—è –∑–∞–ø–æ–ª–Ω–µ–Ω–∏—è –±–∞—Ç—á–∞ (—Å–µ–∫—É–Ω–¥—ã)
            max_cache_size: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –∫—ç—à–∞
            llm_provider: –ü—Ä–æ–≤–∞–π–¥–µ—Ä LLM –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        """
        self.batch_size = batch_size
        self.batch_timeout = batch_timeout
        self.max_cache_size = max_cache_size
        self.default_llm_provider = llm_provider

        # –û—á–µ—Ä–µ–¥—å –∑–∞–ø—Ä–æ—Å–æ–≤
        self.queue: asyncio.Queue[LLMRequest] = asyncio.Queue()
        # –ö—ç—à —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ (cache_key -> (relevance_days, raw_response))
        self.cache: dict[str, tuple[int, str]] = {}
        # –§–ª–∞–≥ —Ä–∞–±–æ—Ç—ã –±–∞—Ç—á–µ—Ä–∞
        self.is_running = False
        # –ó–∞–¥–∞—á–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –±–∞—Ç—á–µ–π
        self.batch_task: asyncio.Task | None = None

    async def start(self):
        """–ó–∞–ø—É—Å–∫–∞–µ—Ç —Ñ–æ–Ω–æ–≤—É—é –∑–∞–¥–∞—á—É –æ–±—Ä–∞–±–æ—Ç–∫–∏ –±–∞—Ç—á–µ–π"""
        if self.is_running:
            logger.warning("‚ö†Ô∏è [llm_batcher] –ë–∞—Ç—á–µ—Ä —É–∂–µ –∑–∞–ø—É—â–µ–Ω")
            return

        self.is_running = True
        self.batch_task = asyncio.create_task(self._batch_processor())
        logger.info(
            f"‚úÖ [llm_batcher] –ë–∞—Ç—á–µ—Ä –∑–∞–ø—É—â–µ–Ω (batch_size={self.batch_size}, "
            f"batch_timeout={self.batch_timeout}s)"
        )

    async def stop(self):
        """–û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç –±–∞—Ç—á–µ—Ä"""
        self.is_running = False
        if self.batch_task:
            self.batch_task.cancel()
            try:
                await self.batch_task
            except asyncio.CancelledError:
                pass
        logger.info("üõë [llm_batcher] –ë–∞—Ç—á–µ—Ä –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")

    async def determine_relevance_days(
        self, post_text: str, llm_provider: str | None = None
    ) -> tuple[int, str]:
        """
        –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–Ω–µ–π –∞–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç–∏ –ø–æ—Å—Ç–∞ —á–µ—Ä–µ–∑ LLM —Å –±–∞—Ç—á–∏–Ω–≥–æ–º

        Args:
            post_text: –¢–µ–∫—Å—Ç –ø–æ—Å—Ç–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            llm_provider: –ü—Ä–æ–≤–∞–π–¥–µ—Ä LLM (–µ—Å–ª–∏ None, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è default)

        Returns:
            tuple[int, str]: (–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–Ω–µ–π –∞–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç–∏, —Å—ã—Ä–æ–π –æ—Ç–≤–µ—Ç LLM)
        """
        provider = llm_provider or self.default_llm_provider

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
        cache_key = f"{provider}:{hashlib.md5(post_text.encode()).hexdigest()}"
        if cache_key in self.cache:
            relevance_days, raw_response = self.cache[cache_key]
            logger.debug(f"üíæ [llm_batcher] –†–µ–∑—É–ª—å—Ç–∞—Ç –∏–∑ –∫—ç—à–∞ –¥–ª—è –ø–æ—Å—Ç–∞ (–¥–ª–∏–Ω–∞: {len(post_text)} —Å–∏–º–≤–æ–ª–æ–≤)")
            return relevance_days, raw_response

        # –°–æ–∑–¥–∞–µ–º –∑–∞–ø—Ä–æ—Å
        request = LLMRequest(post_text=post_text, llm_provider=provider, cache_key=cache_key)

        # –î–æ–±–∞–≤–ª—è–µ–º –≤ –æ—á–µ—Ä–µ–¥—å
        await self.queue.put(request)

        # –ñ–¥–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
        try:
            relevance_days, raw_response = await asyncio.wait_for(request.future, timeout=30.0)
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫—ç—à
            self._add_to_cache(cache_key, relevance_days, raw_response)
            return relevance_days, raw_response
        except asyncio.TimeoutError:
            logger.error(f"‚ùå [llm_batcher] –¢–∞–π–º–∞—É—Ç –æ–∂–∏–¥–∞–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –¥–ª—è –ø–æ—Å—Ç–∞ (–¥–ª–∏–Ω–∞: {len(post_text)} —Å–∏–º–≤–æ–ª–æ–≤)")
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –∑–Ω–∞—á–µ–Ω–∏–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
            return 30, "TIMEOUT"

    async def _batch_processor(self):
        """–§–æ–Ω–æ–≤–∞—è –∑–∞–¥–∞—á–∞ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –±–∞—Ç—á–µ–π"""
        logger.info("üîÑ [llm_batcher] –ó–∞–ø—É—â–µ–Ω–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞—Ç—á–µ–π")

        while self.is_running:
            try:
                # –°–æ–±–∏—Ä–∞–µ–º –±–∞—Ç—á –∑–∞–ø—Ä–æ—Å–æ–≤
                batch = await self._collect_batch()

                if not batch:
                    continue

                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –±–∞—Ç—á
                await self._process_batch(batch)

            except asyncio.CancelledError:
                logger.info("üõë [llm_batcher] –û–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞—Ç—á–µ–π –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞")
                break
            except Exception as e:
                logger.error(f"‚ùå [llm_batcher] –û—à–∏–±–∫–∞ –≤ –æ–±—Ä–∞–±–æ—Ç–∫–µ –±–∞—Ç—á–µ–π: {e}", exc_info=True)
                await asyncio.sleep(1)  # –ù–µ–±–æ–ª—å—à–∞—è –ø–∞—É–∑–∞ –ø–µ—Ä–µ–¥ —Å–ª–µ–¥—É—é—â–µ–π –ø–æ–ø—ã—Ç–∫–æ–π

    async def _collect_batch(self) -> list[LLMRequest]:
        """
        –°–æ–±–∏—Ä–∞–µ—Ç –±–∞—Ç—á –∑–∞–ø—Ä–æ—Å–æ–≤ –∏–∑ –æ—á–µ—Ä–µ–¥–∏

        Returns:
            –°–ø–∏—Å–æ–∫ –∑–∞–ø—Ä–æ—Å–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
        """
        batch: list[LLMRequest] = []
        first_request = None

        # –ñ–¥–µ–º –ø–µ—Ä–≤—ã–π –∑–∞–ø—Ä–æ—Å
        try:
            first_request = await asyncio.wait_for(self.queue.get(), timeout=self.batch_timeout)
            batch.append(first_request)
        except asyncio.TimeoutError:
            return []

        # –°–æ–±–∏—Ä–∞–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã –¥–æ –∑–∞–ø–æ–ª–Ω–µ–Ω–∏—è –±–∞—Ç—á–∞ –∏–ª–∏ —Ç–∞–π–º–∞—É—Ç–∞
        batch_timeout = self.batch_timeout
        while len(batch) < self.batch_size:
            try:
                request = await asyncio.wait_for(self.queue.get(), timeout=batch_timeout)
                batch.append(request)
                # –£–º–µ–Ω—å—à–∞–µ–º —Ç–∞–π–º–∞—É—Ç –¥–ª—è —Å–ª–µ–¥—É—é—â–∏—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
                batch_timeout = 0.1
            except asyncio.TimeoutError:
                break

        logger.debug(f"üì¶ [llm_batcher] –°–æ–±—Ä–∞–Ω –±–∞—Ç—á –∏–∑ {len(batch)} –∑–∞–ø—Ä–æ—Å–æ–≤")
        return batch

    async def _process_batch(self, batch: list[LLMRequest]):
        """
        –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –±–∞—Ç—á –∑–∞–ø—Ä–æ—Å–æ–≤

        Args:
            batch: –°–ø–∏—Å–æ–∫ –∑–∞–ø—Ä–æ—Å–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
        """
        if not batch:
            return

        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å—ã –ø–æ –ø—Ä–æ–≤–∞–π–¥–µ—Ä—É
        requests_by_provider: dict[str, list[LLMRequest]] = {}
        for request in batch:
            provider = request.llm_provider
            if provider not in requests_by_provider:
                requests_by_provider[provider] = []
            requests_by_provider[provider].append(request)

        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—É—é –≥—Ä—É–ø–ø—É –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
        tasks = []
        for provider, provider_requests in requests_by_provider.items():
            task = self._process_provider_batch(provider, provider_requests)
            tasks.append(task)

        await asyncio.gather(*tasks, return_exceptions=True)

    async def _process_provider_batch(self, provider: str, requests: list[LLMRequest]):
        """
        –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –±–∞—Ç—á –∑–∞–ø—Ä–æ—Å–æ–≤ –¥–ª—è –æ–¥–Ω–æ–≥–æ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞

        Args:
            provider: –ü—Ä–æ–≤–∞–π–¥–µ—Ä LLM
            requests: –°–ø–∏—Å–æ–∫ –∑–∞–ø—Ä–æ—Å–æ–≤
        """
        try:
            llm_client = get_llm(provider)

            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∑–∞–ø—Ä–æ—Å—ã –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ (–Ω–æ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º)
            # –î–ª—è –∫–∞–∂–¥–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞ —Å–æ–∑–¥–∞–µ–º –æ—Ç–¥–µ–ª—å–Ω—ã–π –≤—ã–∑–æ–≤ LLM
            # –í –±—É–¥—É—â–µ–º –º–æ–∂–Ω–æ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å –¥–ª—è –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤, –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—â–∏—Ö batch API
            tasks = []
            for request in requests:
                task = self._process_single_request(llm_client, request, provider)
                tasks.append(task)

            await asyncio.gather(*tasks, return_exceptions=True)

        except Exception as e:
            logger.error(f"‚ùå [llm_batcher] –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –±–∞—Ç—á–∞ –¥–ª—è –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞ {provider}: {e}", exc_info=True)
            # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –æ—à–∏–±–∫—É –¥–ª—è –≤—Å–µ—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –≤ –±–∞—Ç—á–µ
            for request in requests:
                if not request.future.done():
                    request.future.set_exception(e)

    async def _process_single_request(
        self, llm_client: Any, request: LLMRequest, provider: str
    ):
        """
        –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ–¥–∏–Ω –∑–∞–ø—Ä–æ—Å –∫ LLM

        Args:
            llm_client: –ö–ª–∏–µ–Ω—Ç LLM
            request: –ó–∞–ø—Ä–æ—Å
            provider: –ü—Ä–æ–≤–∞–π–¥–µ—Ä LLM
        """
        try:
            from tplexity.tg_parse.relevance_analyzer import RELEVANCE_PROMPT

            messages = [
                {
                    "role": "user",
                    "content": RELEVANCE_PROMPT.format(post_text=request.post_text),
                }
            ]

            raw_response = await llm_client.generate(
                messages=messages,
                temperature=0.0,
                max_tokens=50,
            )

            # –ò–∑–≤–ª–µ–∫–∞–µ–º —á–∏—Å–ª–æ –∏–∑ –æ—Ç–≤–µ—Ç–∞
            response = raw_response.strip()
            digits = ""
            for char in response:
                if char.isdigit():
                    digits += char
                elif digits:
                    break

            if not digits:
                logger.warning(
                    f"‚ö†Ô∏è [llm_batcher] –ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —á–∏—Å–ª–æ –∏–∑ –æ—Ç–≤–µ—Ç–∞ LLM: {response}, "
                    f"–∏—Å–ø–æ–ª—å–∑—É–µ–º –∑–Ω–∞—á–µ–Ω–∏–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 30"
                )
                relevance_days = 30
            else:
                relevance_days = int(digits)
                # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–∏–∞–ø–∞–∑–æ–Ω –æ—Ç 1 –¥–æ 10000
                relevance_days = max(1, min(10000, relevance_days))

            # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            if not request.future.done():
                request.future.set_result((relevance_days, raw_response))

            logger.debug(
                f"‚úÖ [llm_batcher] –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∞ –∞–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç—å: {relevance_days} –¥–Ω–µ–π "
                f"–¥–ª—è –ø–æ—Å—Ç–∞ (–¥–ª–∏–Ω–∞: {len(request.post_text)} —Å–∏–º–≤–æ–ª–æ–≤)"
            )

        except Exception as e:
            logger.error(
                f"‚ùå [llm_batcher] –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∑–∞–ø—Ä–æ—Å–∞: {e}",
                exc_info=True,
            )
            # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∑–Ω–∞—á–µ–Ω–∏–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –ø—Ä–∏ –æ—à–∏–±–∫–µ
            if not request.future.done():
                request.future.set_result((30, f"ERROR: {str(e)}"))

    def _add_to_cache(self, cache_key: str, relevance_days: int, raw_response: str):
        """
        –î–æ–±–∞–≤–ª—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ –∫—ç—à —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º —Ä–∞–∑–º–µ—Ä–∞

        Args:
            cache_key: –ö–ª—é—á –∫—ç—à–∞
            relevance_days: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–Ω–µ–π –∞–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç–∏
            raw_response: –°—ã—Ä–æ–π –æ—Ç–≤–µ—Ç LLM
        """
        # –ï—Å–ª–∏ –∫—ç—à –ø–µ—Ä–µ–ø–æ–ª–Ω–µ–Ω, —É–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ –∑–∞–ø–∏—Å–∏ (FIFO)
        if len(self.cache) >= self.max_cache_size:
            # –£–¥–∞–ª—è–µ–º –ø–µ—Ä–≤—É—é –∑–∞–ø–∏—Å—å (—Å–∞–º—É—é —Å—Ç–∞—Ä—É—é)
            oldest_key = next(iter(self.cache))
            del self.cache[oldest_key]

        self.cache[cache_key] = (relevance_days, raw_response)


# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä –±–∞—Ç—á–µ—Ä–∞ (singleton)
_batcher_instance: LLMBatcher | None = None


def get_batcher(llm_provider: str = "qwen") -> LLMBatcher:
    """
    –ü–æ–ª—É—á–∏—Ç—å –≥–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä –±–∞—Ç—á–µ—Ä–∞ (singleton)

    Args:
        llm_provider: –ü—Ä–æ–≤–∞–π–¥–µ—Ä LLM –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é

    Returns:
        LLMBatcher: –≠–∫–∑–µ–º–ø–ª—è—Ä –±–∞—Ç—á–µ—Ä–∞

    –ü—Ä–∏–º–µ—á–∞–Ω–∏–µ:
        –ë–∞—Ç—á–µ—Ä –Ω—É–∂–Ω–æ –∑–∞–ø—É—Å—Ç–∏—Ç—å –≤—Ä—É—á–Ω—É—é —á–µ—Ä–µ–∑ await batcher.start()
        –ø–æ—Å–ª–µ –ø–æ–ª—É—á–µ–Ω–∏—è —ç–∫–∑–µ–º–ø–ª—è—Ä–∞
    """
    global _batcher_instance

    if _batcher_instance is None:
        _batcher_instance = LLMBatcher(llm_provider=llm_provider)

    return _batcher_instance

