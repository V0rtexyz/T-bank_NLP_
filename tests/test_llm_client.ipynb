{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤ LLM\n",
    "\n",
    "–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ –ø—Ä–æ–≤–∞–π–¥–µ—Ä—ã:\n",
    "- **Qwen** - –ª–æ–∫–∞–ª—å–Ω—ã–π/—É–¥–∞–ª–µ–Ω–Ω—ã–π Qwen API\n",
    "- **YandexGPT** - Yandex Cloud LLM API\n",
    "- **ChatGPT** - OpenAI API\n",
    "- **DeepSeek** - DeepSeek API\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ –ö–æ—Ä–Ω–µ–≤–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –ø—Ä–æ–µ–∫—Ç–∞: /Users/avolon/Documents/SIRIUS/T-bank_NLP_\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "print(f\"üìÅ –ö–æ—Ä–Ω–µ–≤–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –ø—Ä–æ–µ–∫—Ç–∞: {project_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ –ò–º–ø–æ—Ä—Ç—ã –≤—ã–ø–æ–ª–Ω–µ–Ω—ã —É—Å–ø–µ—à–Ω–æ\n"
     ]
    }
   ],
   "source": [
    "from tplexity.llm_client import get_llm\n",
    "\n",
    "print(\"‚úÖ –ò–º–ø–æ—Ä—Ç—ã –≤—ã–ø–æ–ª–Ω–µ–Ω—ã —É—Å–ø–µ—à–Ω–æ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–¢–µ—Å—Ç–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å:\n",
      "–°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç: –¢—ã - –ø–æ–ª–µ–∑–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç.\n",
      "–°–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: –ü—Ä–∏–≤–µ—Ç!\n",
      "–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {'temperature': 0.7, 'max_tokens': 500}\n"
     ]
    }
   ],
   "source": [
    "TEST_MESSAGES = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"–¢—ã - –ø–æ–ª–µ–∑–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"–ü—Ä–∏–≤–µ—Ç!\",\n",
    "    },\n",
    "]\n",
    "\n",
    "GENERATION_PARAMS = {\"temperature\": 0.7, \"max_tokens\": 500}\n",
    "\n",
    "print(\"–¢–µ—Å—Ç–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å:\")\n",
    "print(f\"–°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç: {TEST_MESSAGES[0]['content']}\")\n",
    "print(f\"–°–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {TEST_MESSAGES[1]['content']}\")\n",
    "print(f\"–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {GENERATION_PARAMS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def test_provider(\n",
    "    provider: str,\n",
    "    messages: list[dict],\n",
    "    generation_params: dict,\n",
    ") -> dict[str, Any]:\n",
    "    \"\"\"\n",
    "    –¢–µ—Å—Ç–∏—Ä—É–µ—Ç —Ä–∞–±–æ—Ç—É LLM –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–º –∑–∞–ø—Ä–æ—Å–µ\n",
    "\n",
    "    Args:\n",
    "        provider (str): –ù–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞\n",
    "        messages (list[dict]): –°–ø–∏—Å–æ–∫ —Å–æ–æ–±—â–µ–Ω–∏–π –≤ —Ñ–æ—Ä–º–∞—Ç–µ OpenAI (—Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç + —Å–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è)\n",
    "        generation_params (dict): –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏\n",
    "\n",
    "    Returns:\n",
    "        –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è\n",
    "    \"\"\"\n",
    "    result = {\"provider\": provider, \"status\": \"unknown\", \"response\": None, \"time\": 0, \"error\": None}\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    try:\n",
    "        print(f\"–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞: {provider.upper()}\")\n",
    "\n",
    "        try:\n",
    "            client = get_llm(provider)\n",
    "            print(f\"‚úÖ –ö–ª–∏–µ–Ω—Ç –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω: model={client.model}\")\n",
    "        except Exception as e:\n",
    "            error_msg = f\"–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –∫–ª–∏–µ–Ω—Ç–∞: {e}\"\n",
    "            result[\"error\"] = error_msg\n",
    "            result[\"status\"] = \"failed\"\n",
    "            print(f\"‚ùå {error_msg}\")\n",
    "            return result\n",
    "\n",
    "        try:\n",
    "            response = await client.generate(\n",
    "                messages=messages,\n",
    "                temperature=generation_params.get(\"temperature\"),\n",
    "                max_tokens=generation_params.get(\"max_tokens\"),\n",
    "            )\n",
    "\n",
    "            result[\"response\"] = response\n",
    "            result[\"status\"] = \"success\"\n",
    "            result[\"time\"] = time.time() - start_time\n",
    "\n",
    "            print(f\"‚úÖ –£—Å–ø–µ—à–Ω–æ –∑–∞ {result['time']:.2f}—Å\")\n",
    "            print(f\"–û—Ç–≤–µ—Ç: {response}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            result[\"status\"] = \"failed\"\n",
    "            result[\"error\"] = str(e)\n",
    "            result[\"time\"] = time.time() - start_time\n",
    "            print(f\"‚ùå –û—à–∏–±–∫–∞: {e}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        result[\"status\"] = \"failed\"\n",
    "        result[\"error\"] = f\"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}\"\n",
    "        result[\"time\"] = time.time() - start_time\n",
    "        print(f\"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}\")\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-16 16:59:44,749 - tplexity.llm_client.client - INFO - üîÑ [llm_client] –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è LLM –∫–ª–∏–µ–Ω—Ç–∞: model=QuantTrio/Qwen3-VL-30B-A3B-Instruct-AWQ, base_url=http://195.209.210.28:8000/v1\n",
      "2025-11-16 16:59:44,768 - tplexity.llm_client.client - INFO - ‚úÖ [llm_client] LLM –∫–ª–∏–µ–Ω—Ç –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞: QWEN\n",
      "‚úÖ –ö–ª–∏–µ–Ω—Ç –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω: model=QuantTrio/Qwen3-VL-30B-A3B-Instruct-AWQ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-16 16:59:45,146 - httpx - INFO - HTTP Request: POST http://195.209.210.28:8000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-16 16:59:45,155 - tplexity.llm_client.client - INFO - ‚úÖ [llm_client] –û—Ç–≤–µ—Ç –ø–æ–ª—É—á–µ–Ω –æ—Ç LLM\n",
      "2025-11-16 16:59:45,155 - tplexity.llm_client.client - INFO - üîÑ [llm_client] –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è LLM –∫–ª–∏–µ–Ω—Ç–∞: model=gpt://b1g737e6b62eqjfkjd8l/yandexgpt-lite, base_url=https://llm.api.cloud.yandex.net/v1\n",
      "2025-11-16 16:59:45,165 - tplexity.llm_client.client - INFO - ‚úÖ [llm_client] LLM –∫–ª–∏–µ–Ω—Ç –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ –£—Å–ø–µ—à–Ω–æ –∑–∞ 0.41—Å\n",
      "–û—Ç–≤–µ—Ç: –ü—Ä–∏–≤–µ—Ç! –ö–∞–∫ —è –º–æ–≥—É —Ç–µ–±–µ –ø–æ–º–æ—á—å? üòä\n",
      "\n",
      "–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞: YANDEXGPT\n",
      "‚úÖ –ö–ª–∏–µ–Ω—Ç –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω: model=gpt://b1g737e6b62eqjfkjd8l/yandexgpt-lite\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-16 16:59:45,750 - httpx - INFO - HTTP Request: POST https://llm.api.cloud.yandex.net/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-16 16:59:45,753 - tplexity.llm_client.client - INFO - ‚úÖ [llm_client] –û—Ç–≤–µ—Ç –ø–æ–ª—É—á–µ–Ω –æ—Ç LLM\n",
      "2025-11-16 16:59:45,753 - tplexity.llm_client.client - INFO - üîÑ [llm_client] –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è LLM –∫–ª–∏–µ–Ω—Ç–∞: model=gpt-4o-mini, base_url=None\n",
      "2025-11-16 16:59:45,767 - tplexity.llm_client.client - INFO - ‚úÖ [llm_client] LLM –∫–ª–∏–µ–Ω—Ç –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ –£—Å–ø–µ—à–Ω–æ –∑–∞ 0.60—Å\n",
      "–û—Ç–≤–µ—Ç: –ó–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ! –ß–µ–º —è –º–æ–≥—É –≤–∞–º –ø–æ–º–æ—á—å?\n",
      "\n",
      "–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞: CHATGPT\n",
      "‚úÖ –ö–ª–∏–µ–Ω—Ç –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω: model=gpt-4o-mini\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-16 16:59:45,991 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 403 Forbidden\"\n",
      "2025-11-16 16:59:45,994 - tplexity.llm_client.client - ERROR - ‚ùå [llm_client] –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–∑–æ–≤–µ LLM: Error code: 403 - {'error': {'code': 'unsupported_country_region_territory', 'message': 'Country, region, or territory not supported', 'param': None, 'type': 'request_forbidden'}}\n",
      "2025-11-16 16:59:45,995 - tplexity.llm_client.client - INFO - üîÑ [llm_client] –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è LLM –∫–ª–∏–µ–Ω—Ç–∞: model=deepseek-chat, base_url=https://api.deepseek.com\n",
      "2025-11-16 16:59:46,005 - tplexity.llm_client.client - INFO - ‚úÖ [llm_client] LLM –∫–ª–∏–µ–Ω—Ç –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå –û—à–∏–±–∫–∞: Error code: 403 - {'error': {'code': 'unsupported_country_region_territory', 'message': 'Country, region, or territory not supported', 'param': None, 'type': 'request_forbidden'}}\n",
      "\n",
      "–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞: DEEPSEEK\n",
      "‚úÖ –ö–ª–∏–µ–Ω—Ç –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω: model=deepseek-chat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-16 16:59:46,825 - httpx - INFO - HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-16 16:59:48,606 - tplexity.llm_client.client - INFO - ‚úÖ [llm_client] –û—Ç–≤–µ—Ç –ø–æ–ª—É—á–µ–Ω –æ—Ç LLM\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ –£—Å–ø–µ—à–Ω–æ –∑–∞ 2.61—Å\n",
      "–û—Ç–≤–µ—Ç: –ü—Ä–∏–≤–µ—Ç! üòä –†–∞–¥ —Ç–µ–±—è –≤–∏–¥–µ—Ç—å! –ö–∞–∫ —É —Ç–µ–±—è —Å–µ–≥–æ–¥–Ω—è –¥–µ–ª–∞? –ß–µ–º –º–æ–≥—É –ø–æ–º–æ—á—å?\n",
      "\n",
      "–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ\n"
     ]
    }
   ],
   "source": [
    "PROVIDERS = [\"qwen\", \"yandexgpt\", \"chatgpt\", \"deepseek\"]\n",
    "\n",
    "all_results = {}\n",
    "\n",
    "for provider in PROVIDERS:\n",
    "    result = await test_provider(\n",
    "        provider=provider,\n",
    "        messages=TEST_MESSAGES,\n",
    "        generation_params=GENERATION_PARAMS,\n",
    "    )\n",
    "    all_results[provider] = result\n",
    "    print()\n",
    "\n",
    "print(\"–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
